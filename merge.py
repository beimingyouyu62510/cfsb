import requests
import yaml
import os
import base64
import json
from hashlib import md5
import sys
import time
import urllib.parse
import asyncio
import aiohttp
import socket
import concurrent.futures
from aiohttp import client_exceptions
import statistics
from datetime import datetime, timedelta
import random
import ipaddress
from collections import defaultdict
import re
import subprocess
import tempfile

# ========== é…ç½®ï¼šå›ºå®šæ›´æ–°æ–‡ä»¶ URL å’Œæ–‡ä»¶è·¯å¾„ ==========
UPDATE_FILE_URL = "https://apicsv.sosorg.nyc.mn/gengxin.txt?token=CMorg"
FALLBACK_FILE = "fallback_urls.txt"
OUTPUT_ALL = "providers/all.yaml"
OUTPUT_US = "providers/us.yaml"
QUALITY_REPORT = "quality_report.json"
BLACKLIST_FILE = "blacklist_ips.txt"

# ä¼˜åŒ–åçš„æµ‹è¯•é…ç½® - æ›´å®½æ¾ä½†æ›´å®ç”¨
TEST_URLS = [
    "http://cp.cloudflare.com/generate_204",
    "http://www.google.com/generate_204", 
    "http://detectportal.firefox.com/success.txt",
    "http://httpbin.org/ip"  # æ·»åŠ IPæ£€æµ‹URL
]
TEST_TIMEOUT = 25  # å¢åŠ è¶…æ—¶æ—¶é—´
MAX_CONCURRENCY = 15  # é™ä½å¹¶å‘æ•°æé«˜ç¨³å®šæ€§
RETRY_COUNT = 2
LATENCY_THRESHOLD = 3000  # æ”¾å®½å»¶è¿Ÿé˜ˆå€¼åˆ°3ç§’
SUCCESS_RATE_THRESHOLD = 0.3  # é™ä½æˆåŠŸç‡è¦æ±‚åˆ°30%
MAX_NODES_PER_IP = 5  # å¢åŠ æ¯ä¸ªIPçš„èŠ‚ç‚¹æ•°é‡é™åˆ¶
MIN_QUALITY_SCORE = 25  # å¤§å¹…é™ä½æœ€ä½è´¨é‡åˆ†æ•°è¦æ±‚

# å‘½åé…ç½®
NAMING_CONFIG = {
    "PRESERVE_ORIGINAL_NAMES": True,  # ä¿ç•™åŸå§‹åç§°
    "CLEAN_JUNK_CHARS": True,         # æ¸…ç†åƒåœ¾å­—ç¬¦
    "ADD_LOCATION_PREFIX": False,     # ä¸æ·»åŠ åœ°ç†ä½ç½®å‰ç¼€
    "MAX_NAME_LENGTH": 80,            # å¢åŠ æœ€å¤§åç§°é•¿åº¦
    "REMOVE_TEST_WARNINGS": True,     # ç§»é™¤æµ‹é€Ÿè­¦å‘Š
}

# è´¨é‡è¯„åˆ†æƒé‡ - æ›´æ³¨é‡è¿é€šæ€§è€Œéé€Ÿåº¦
WEIGHTS = {
    'connectivity': 0.50,    # è¿é€šæ€§æƒé‡æœ€é«˜
    'latency': 0.25,         # é™ä½å»¶è¿Ÿæƒé‡
    'success_rate': 0.20,    # æˆåŠŸç‡æƒé‡
    'stability': 0.05        # ç¨³å®šæ€§æƒé‡æœ€ä½
}

# ========== IP è´¨é‡ç®¡ç† ==========
def load_ip_blacklist():
    """åŠ è½½IPé»‘åå•"""
    if os.path.exists(BLACKLIST_FILE):
        with open(BLACKLIST_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_ip_blacklist(blacklist):
    """ä¿å­˜IPé»‘åå•"""
    with open(BLACKLIST_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(blacklist)))

def is_valid_ip(ip):
    """éªŒè¯IPåœ°å€æœ‰æ•ˆæ€§ï¼Œä½†ä¸æ’é™¤æ‰€æœ‰å†…ç½‘IPï¼ˆæŸäº›ä»£ç†å¯èƒ½ä½¿ç”¨ï¼‰"""
    try:
        ip_obj = ipaddress.ip_address(ip)
        # åªæ’é™¤å›ç¯ã€å¤šæ’­ç­‰æ˜æ˜¾æ— æ•ˆçš„IP
        if ip_obj.is_loopback or ip_obj.is_multicast or ip_obj.is_reserved:
            return False
        return True
    except:
        return False

def analyze_current_nodes(yaml_file):
    """åˆ†æå½“å‰èŠ‚ç‚¹è´¨é‡é—®é¢˜"""
    try:
        with open(yaml_file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            proxies = data.get('proxies', [])
        
        print(f"\n=== å½“å‰èŠ‚ç‚¹åˆ†æ ===")
        print(f"æ€»èŠ‚ç‚¹æ•°: {len(proxies)}")
        
        # ç»Ÿè®¡IPä½¿ç”¨æƒ…å†µ
        ip_count = defaultdict(int)
        uuid_count = defaultdict(int)
        name_patterns = defaultdict(int)
        
        for proxy in proxies:
            ip = proxy.get('server', '')
            uuid = proxy.get('uuid', '')
            name = proxy.get('name', '')
            
            ip_count[ip] += 1
            uuid_count[uuid] += 1
            # æå–åç§°æ¨¡å¼
            base_name = name.split('ã€')[0] if 'ã€' in name else name
            name_patterns[base_name] += 1
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"å”¯ä¸€IPæ•°é‡: {len(ip_count)}")
        print(f"å”¯ä¸€UUIDæ•°é‡: {len(uuid_count)}")
        
        # é‡å¤IPè¿‡å¤šçš„æƒ…å†µ
        high_repeat_ips = {ip: count for ip, count in ip_count.items() if count > 8}
        if high_repeat_ips:
            print(f"âš ï¸ é«˜é‡å¤IP ({len(high_repeat_ips)}ä¸ª):")
            for ip, count in sorted(high_repeat_ips.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  {ip}: {count}ä¸ªèŠ‚ç‚¹")
        
        # UUIDé‡å¤æƒ…å†µ
        if len(uuid_count) < 5:
            print(f"âš ï¸ UUIDå¤šæ ·æ€§ä¸è¶³ï¼Œåªæœ‰{len(uuid_count)}ä¸ªä¸åŒUUID")
        
        return proxies, ip_count, uuid_count
        
    except Exception as e:
        print(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
        return [], {}, {}

# ========== å¢å¼ºçš„èŠ‚ç‚¹éªŒè¯å’Œç­›é€‰ ==========
def enhanced_validate_proxy(proxy):
    """å¢å¼ºçš„ä»£ç†é…ç½®éªŒè¯ï¼Œæ›´å®½æ¾çš„éªŒè¯è§„åˆ™"""
    required_fields = ['name', 'type', 'server', 'port']
    
    # åŸºç¡€å­—æ®µæ£€æŸ¥
    for field in required_fields:
        if field not in proxy or not proxy[field]:
            return False, f"ç¼ºå°‘å­—æ®µ: {field}"
    
    # ç«¯å£éªŒè¯
    try:
        port = int(proxy['port'])
        if port <= 0 or port > 65535:
            return False, f"ç«¯å£èŒƒå›´é”™è¯¯: {port}"
    except:
        return False, "ç«¯å£ä¸æ˜¯æ•°å­—"
    
    # IPåœ°å€éªŒè¯ï¼ˆæ›´å®½æ¾ï¼‰
    server = proxy.get('server', '')
    if not is_valid_ip(server):
        # å…è®¸åŸŸå
        if not re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', server):
            return False, f"æ— æ•ˆæœåŠ¡å™¨åœ°å€: {server}"
    
    # UUIDéªŒè¯ï¼ˆæ›´å®½æ¾ï¼‰
    if proxy.get('type') == 'vless':
        uuid = proxy.get('uuid', '')
        if len(uuid) < 20:  # æ”¾å®½UUIDè¦æ±‚
            return False, f"UUIDè¿‡çŸ­: {uuid}"
    
    return True, "valid"

def intelligent_dedup(proxies):
    """æ™ºèƒ½å»é‡ï¼Œä¿ç•™æ›´å¤šèŠ‚ç‚¹"""
    # æŒ‰æœåŠ¡å™¨IPåˆ†ç»„
    ip_groups = defaultdict(list)
    for proxy in proxies:
        ip = proxy.get('server', '')
        ip_groups[ip].append(proxy)
    
    result = []
    blacklist = load_ip_blacklist()
    
    for ip, group in ip_groups.items():
        # è·³è¿‡é»‘åå•IP
        if ip in blacklist:
            print(f"[âš ï¸] è·³è¿‡é»‘åå•IP: {ip}")
            continue
            
        # å¢åŠ æ¯ä¸ªIPçš„èŠ‚ç‚¹æ•°é‡é™åˆ¶
        if len(group) > MAX_NODES_PER_IP:
            print(f"[ğŸ“Š] IP {ip} æœ‰ {len(group)} ä¸ªèŠ‚ç‚¹ï¼Œé™åˆ¶ä¸º {MAX_NODES_PER_IP} ä¸ª")
            # æŒ‰ç«¯å£å’Œé…ç½®å¤šæ ·æ€§æ’åºï¼Œä¿ç•™ä¸åŒé…ç½®çš„èŠ‚ç‚¹
            group.sort(key=lambda x: (x.get('port', 0), x.get('servername', ''), str(x.get('ws-opts', {}))))
            group = group[:MAX_NODES_PER_IP]
        
        result.extend(group)
    
    print(f"[DEBUG] æ™ºèƒ½å»é‡åèŠ‚ç‚¹æ•°: {len(result)} (åŸå§‹: {len(proxies)})")
    return result

def enhanced_us_filter(proxies):
    """å¢å¼ºçš„USèŠ‚ç‚¹ç­›é€‰ï¼Œæ›´ç²¾ç¡®è¯†åˆ«"""
    us_nodes = []
    
    # æ‰©å±•çš„æ’é™¤å…³é”®è¯
    exclude_keywords = [
        # äºšæ´²
        "HK", "HONG KONG", "é¦™æ¸¯", "æ¸¯", "HONGKONG",
        "SG", "SINGAPORE", "æ–°åŠ å¡", "ç‹®åŸ",
        "JP", "JAPAN", "æ—¥æœ¬", "ä¸œäº¬", "TOKYO", "OSAKA",
        "KR", "KOREA", "éŸ©å›½", "é¦–å°”", "SEOUL",
        "TW", "TAIWAN", "å°æ¹¾", "å°åŒ—", "TAIPEI",
        "CN", "CHINA", "ä¸­å›½", "å¤§é™†", "MAINLAND",
        "MY", "MALAYSIA", "é©¬æ¥è¥¿äºš",
        "TH", "THAILAND", "æ³°å›½",
        "VN", "VIETNAM", "è¶Šå—",
        "IN", "INDIA", "å°åº¦",
        # æ¬§æ´²
        "UK", "LONDON", "è‹±å›½", "ä¼¦æ•¦", "BRITAIN",
        "DE", "GERMANY", "å¾·å›½", "æ³•å…°å…‹ç¦", "FRANKFURT",
        "FR", "FRANCE", "æ³•å›½", "å·´é»", "PARIS",
        "NL", "NETHERLANDS", "è·å…°", "AMSTERDAM",
        "IT", "ITALY", "æ„å¤§åˆ©",
        "ES", "SPAIN", "è¥¿ç­ç‰™",
        "RU", "RUSSIA", "ä¿„ç½—æ–¯", "è«æ–¯ç§‘",
        "TR", "TURKEY", "åœŸè€³å…¶",
        "PL", "POLAND", "æ³¢å…°",
        # å…¶ä»–
        "CA", "CANADA", "åŠ æ‹¿å¤§", "TORONTO",
        "AU", "AUSTRALIA", "æ¾³å¤§åˆ©äºš",
        "BR", "BRAZIL", "å·´è¥¿",
    ]
    
    # ç¾å›½å…³é”®è¯ - æ›´å…¨é¢
    us_keywords = [
        "US", "USA", "ç¾å›½", "UNITED STATES", "AMERICA", "AMERICAN",
        # ä¸»è¦åŸå¸‚
        "LOS ANGELES", "NEW YORK", "CHICAGO", "DALLAS", "HOUSTON",
        "SAN FRANCISCO", "SEATTLE", "MIAMI", "DENVER", "ATLANTA",
        "BOSTON", "PHILADELPHIA", "PHOENIX", "SAN DIEGO", "SAN JOSE",
        "AUSTIN", "COLUMBUS", "FORT WORTH", "CHARLOTTE", "DETROIT",
        "EL PASO", "MEMPHIS", "BALTIMORE", "MILWAUKEE", "ALBUQUERQUE",
        # å·å
        "VIRGINIA", "CALIFORNIA", "TEXAS", "OREGON", "FLORIDA",
        "WASHINGTON", "NEVADA", "ARIZONA", "COLORADO", "GEORGIA",
        "ILLINOIS", "OHIO", "PENNSYLVANIA", "MICHIGAN", "TENNESSEE",
        # ç¼©å†™
        "LA", "NYC", "SF", "DC", "VA", "CA", "TX", "FL", "WA"
    ]
    
    for proxy in proxies:
        name = proxy.get("name", "").upper()
        server = proxy.get("server", "")
        
        # æ£€æŸ¥åç§°ä¸­çš„å…³é”®è¯
        has_us_keyword = any(keyword in name for keyword in us_keywords)
        has_exclude_keyword = any(exclude in name for exclude in exclude_keywords)
        
        # åŸºäºIPåœ°å€çš„åœ°ç†ä½ç½®æ¨æ–­
        ip_seems_us = is_likely_us_ip(server)
        
        if (has_us_keyword and not has_exclude_keyword) or ip_seems_us:
            if not has_exclude_keyword:  # å³ä½¿IPåƒç¾å›½ï¼Œä¹Ÿè¦æ’é™¤æ˜ç¡®æ ‡è®°ä¸ºå…¶ä»–å›½å®¶çš„
                us_nodes.append(proxy)
            else:
                print(f"[âš ï¸] IPä¼¼ä¹æ˜¯ç¾å›½ä½†åç§°æ˜¾ç¤ºå…¶ä»–åœ°åŒº: {proxy['name']}")
    
    print(f"[DEBUG] ç­›é€‰å‡º {len(us_nodes)} ä¸ª US èŠ‚ç‚¹")
    return us_nodes

def is_likely_us_ip(ip):
    """ç®€å•çš„IPåœ°ç†ä½ç½®æ¨æ–­ - åŸºäºå·²çŸ¥çš„ç¾å›½IPæ®µ"""
    try:
        ip_obj = ipaddress.ip_address(ip)
        ip_int = int(ip_obj)
        
        # ä¸€äº›å·²çŸ¥çš„ç¾å›½IPæ®µ
        us_ranges = [
            # Cloudflare
            (ipaddress.ip_address('1.1.1.0'), ipaddress.ip_address('1.1.1.255')),
            # Google DNS
            (ipaddress.ip_address('8.8.8.0'), ipaddress.ip_address('8.8.8.255')),
            # å¸¸è§ç¾å›½æ•°æ®ä¸­å¿ƒIPæ®µ
            (ipaddress.ip_address('108.0.0.0'), ipaddress.ip_address('108.255.255.255')),
            (ipaddress.ip_address('162.0.0.0'), ipaddress.ip_address('162.255.255.255')),
            (ipaddress.ip_address('154.0.0.0'), ipaddress.ip_address('154.255.255.255')),
        ]
        
        for start, end in us_ranges:
            if int(start) <= ip_int <= int(end):
                return True
                
    except:
        pass
    return False

# ========== ä¼˜åŒ–çš„èŠ‚ç‚¹åç§°å¤„ç† ==========
def clean_node_name(original_name):
    """æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„ä¿¡æ¯"""
    if not original_name:
        return "Unknown"
    
    cleaned_name = original_name
    
    if NAMING_CONFIG["CLEAN_JUNK_CHARS"]:
        # ç§»é™¤æµ‹é€Ÿè­¦å‘Šå’Œè£…é¥°å­—ç¬¦
        junk_patterns = [
            r'ã€è¯·å‹¿æµ‹é€Ÿã€‘',
            r'â„¢ï¸+',
            r'ğŸ²+',
            r'ğŸŒ+', 
            r'ï¼št\.me/\w+',  # ç§»é™¤ç”µæŠ¥ç¾¤é“¾æ¥
            r'HKGâ„¢ï¸+',
        ]
        
        for pattern in junk_patterns:
            cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()
    cleaned_name = re.sub(r'^[^\w]+|[^\w]+$', '', cleaned_name)
    
    # å¦‚æœæ¸…ç†åä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè¿”å›åŸå
    if len(cleaned_name) < 3:
        return original_name[:NAMING_CONFIG["MAX_NAME_LENGTH"]]
    
    # é™åˆ¶é•¿åº¦
    if len(cleaned_name) > NAMING_CONFIG["MAX_NAME_LENGTH"]:
        cleaned_name = cleaned_name[:NAMING_CONFIG["MAX_NAME_LENGTH"]] + "..."
    
    return cleaned_name if cleaned_name else original_name

def ensure_unique_names(proxies):
    """ç¡®ä¿èŠ‚ç‚¹åç§°å”¯ä¸€ï¼Œä½†å°½é‡ä¿æŒåŸå§‹å«ä¹‰"""
    name_counts = defaultdict(int)
    result = []
    
    for proxy in proxies:
        original_name = proxy.get('name', 'Unknown')
        
        if NAMING_CONFIG["PRESERVE_ORIGINAL_NAMES"]:
            base_name = clean_node_name(original_name)
        else:
            # å¦‚æœä¸ä¿ç•™åŸåï¼Œç”Ÿæˆæ ‡å‡†åŒ–åç§°
            server = proxy.get('server', 'unknown')
            port = proxy.get('port', 'unknown')
            base_name = f"VLESS-{server}:{port}"
        
        # å¤„ç†é‡å¤åç§°
        name_counts[base_name] += 1
        if name_counts[base_name] == 1:
            final_name = base_name
        else:
            final_name = f"{base_name} #{name_counts[base_name]}"
        
        proxy['name'] = final_name
        result.append(proxy)
    
    return result

# ========== æ›´å®½æ¾çš„è¿æ¥è´¨é‡æµ‹è¯• ==========
async def relaxed_quality_test(session, proxy_config):
    """æ›´å®½æ¾çš„èŠ‚ç‚¹è´¨é‡æµ‹è¯•ï¼Œé‡ç‚¹æµ‹è¯•åŸºç¡€è¿é€šæ€§"""
    node_name = proxy_config.get('name', 'æœªçŸ¥èŠ‚ç‚¹')
    server = proxy_config.get('server')
    port = int(proxy_config.get('port', 0))
    
    if not server or not port:
        return None, "æ— æ•ˆé…ç½®"
    
    print(f"[ğŸ”] æµ‹è¯•èŠ‚ç‚¹: {node_name} ({server}:{port})")
    
    # 1. Socket è¿æ¥æµ‹è¯• - é™ä½è¦æ±‚
    socket_results = []
    for round_num in range(2):  # å‡å°‘æµ‹è¯•è½®æ•°
        latency = await test_socket_connection(server, port, timeout=15)
        if latency is not None:
            socket_results.append(latency)
        await asyncio.sleep(0.5)  # å¢åŠ è½®æ¬¡é—´éš”
    
    if not socket_results:  # å®Œå…¨æ— æ³•è¿æ¥
        print(f"[âŒ] {node_name} | Socket è¿æ¥å®Œå…¨å¤±è´¥")
        return None, "Socketè¿æ¥å¤±è´¥"
    
    socket_avg = statistics.mean(socket_results)
    
    if socket_avg > LATENCY_THRESHOLD:
        print(f"[âš ï¸] {node_name} | å»¶è¿Ÿè¾ƒé«˜ä½†å¯æ¥å—: {socket_avg:.0f}ms")
        # ä¸ç›´æ¥æ‹’ç»ï¼Œç»§ç»­æµ‹è¯•
    
    # 2. ç®€åŒ–çš„HTTPæµ‹è¯• - åªæµ‹è¯•ä¸€ä¸ªURL
    http_success = False
    test_url = random.choice(TEST_URLS)
    
    for attempt in range(2):
        try:
            timeout = aiohttp.ClientTimeout(total=TEST_TIMEOUT)
            async with session.get(test_url, timeout=timeout) as resp:
                if resp.status in [200, 204]:
                    http_success = True
                    break
            await asyncio.sleep(1)
        except Exception as e:
            continue
    
    # 3. å®½æ¾çš„è´¨é‡è¯„åˆ†
    connectivity_score = 100 if http_success else 0
    if socket_results:
        latency_score = max(0, 100 - (socket_avg / 50))  # æ›´å®½æ¾çš„å»¶è¿Ÿè¯„åˆ†
    else:
        latency_score = 0
        
    # ç»¼åˆè¯„åˆ† - é‡ç‚¹å…³æ³¨è¿é€šæ€§
    quality_score = (
        connectivity_score * WEIGHTS['connectivity'] +
        latency_score * WEIGHTS['latency'] +
        50 * WEIGHTS['success_rate'] +  # åŸºç¡€åˆ†æ•°
        50 * WEIGHTS['stability']       # åŸºç¡€åˆ†æ•°
    )
    
    # å¤§å¹…é™ä½ç­›é€‰æ ‡å‡†
    if socket_results and quality_score >= MIN_QUALITY_SCORE:
        proxy_config['quality_score'] = quality_score
        proxy_config['test_info'] = {
            'avg_latency': round(socket_avg, 2) if socket_results else None,
            'http_success': http_success,
            'test_time': datetime.now().isoformat()
        }
        
        status = "ä¼˜ç§€" if quality_score >= 70 else "è‰¯å¥½" if quality_score >= 50 else "å¯ç”¨"
        print(f"[âœ…] {node_name} | è´¨é‡: {quality_score:.0f} ({status}) | å»¶è¿Ÿ: {socket_avg:.0f}ms | HTTP: {'é€šè¿‡' if http_success else 'å¤±è´¥'}")
        return proxy_config, "success"
    else:
        print(f"[âŒ] {node_name} | è´¨é‡åˆ†æ•°è¿‡ä½: {quality_score:.0f}")
        return None, f"è´¨é‡åˆ†æ•°è¿‡ä½: {quality_score:.0f}"

async def test_socket_connection(server, port, timeout=15):
    """ä¼˜åŒ–çš„å¼‚æ­¥ Socket è¿æ¥æµ‹è¯•"""
    try:
        start_time = time.time()
        _, writer = await asyncio.wait_for(
            asyncio.open_connection(server, port),
            timeout=timeout
        )
        end_time = time.time()
        writer.close()
        await writer.wait_closed()
        return (end_time - start_time) * 1000
    except:
        return None

def save_yaml_optimized(path, proxies):
    """ä¼˜åŒ–çš„YAMLä¿å­˜ï¼Œä¿ç•™åŸå§‹åç§°å¹¶ç¡®ä¿å…¼å®¹æ€§"""
    abs_path = os.path.abspath(path)
    os.makedirs(os.path.dirname(abs_path), exist_ok=True)
    
    if not proxies:
        print(f"[âš ï¸] æ²¡æœ‰å¯ä¿å­˜çš„èŠ‚ç‚¹åˆ° {abs_path}")
        # åˆ›å»ºç©ºæ–‡ä»¶ä»¥é¿å…é”™è¯¯
        with open(abs_path, "w", encoding="utf-8") as f:
            yaml.safe_dump({"proxies": []}, f, allow_unicode=True)
        return
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    sorted_proxies = sorted(proxies, key=lambda x: x.get('quality_score', 0), reverse=True)
    
    # å¤„ç†èŠ‚ç‚¹åç§°
    named_proxies = ensure_unique_names(sorted_proxies)
    
    # æ¸…ç†é…ç½®ï¼Œç§»é™¤æµ‹è¯•æ•°æ®
    clean_proxies = []
    for proxy in named_proxies:
        clean_proxy = {k: v for k, v in proxy.items()
                       if k not in ['quality_score', 'test_info']}
        clean_proxies.append(clean_proxy)
    
    # æ ‡å‡†Clashæ ¼å¼
    output_data = {"proxies": clean_proxies}
    
    try:
        with open(abs_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(output_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"[ğŸ’¾] å·²ä¿å­˜åˆ° {abs_path}ï¼ŒèŠ‚ç‚¹æ•°: {len(clean_proxies)}")
        
        if sorted_proxies and 'quality_score' in sorted_proxies[0]:
            scores = [p.get('quality_score', 0) for p in sorted_proxies]
            avg_score = statistics.mean(scores)
            best_score = max(scores)
            print(f"[ğŸ“Š] è´¨é‡åˆ†æ•°èŒƒå›´: {min(scores):.0f}-{best_score:.0f}ï¼Œå¹³å‡: {avg_score:.1f}")
            
            # æ˜¾ç¤ºå‰3ä¸ªæœ€ä½³èŠ‚ç‚¹
            print(f"[ğŸ†] å‰3ä¸ªæœ€ä½³èŠ‚ç‚¹:")
            for i, proxy in enumerate(sorted_proxies[:3], 1):
                print(f"  {i}. {proxy['name']} (åˆ†æ•°: {proxy.get('quality_score', 0):.0f})")
                
    except Exception as e:
        print(f"[âŒ] ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

# ========== ä»å›ºå®š URL è·å–è®¢é˜…æº ==========
async def fetch_subscription_urls(session):
    """ä»å›ºå®š URL ä¸‹è½½è®¢é˜…æºåˆ—è¡¨"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        timeout = aiohttp.ClientTimeout(total=20)
        async with session.get(UPDATE_FILE_URL, timeout=timeout, headers=headers) as resp:
            resp.raise_for_status()
            content = await resp.text()
            if not content.strip():
                return load_fallback_urls()
            urls = [line.strip() for line in content.splitlines()
                   if line.strip() and line.strip().startswith('http')]
            if urls:
                print(f"[âœ…] è·å– {len(urls)} ä¸ªè®¢é˜…æº")
                save_fallback_urls(urls)
                return urls
            else:
                return load_fallback_urls()
    except Exception as e:
        print(f"[âŒ] è·å–è®¢é˜…æºå¤±è´¥: {e}")
        return load_fallback_urls()

def load_fallback_urls():
    """åŠ è½½æœ¬åœ°ä¿å­˜çš„ fallback URL åˆ—è¡¨"""
    if os.path.exists(FALLBACK_FILE):
        with open(FALLBACK_FILE, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
            print(f"[ğŸ’¾] ä»æœ¬åœ°åŠ è½½ {len(urls)} ä¸ªå¤‡ç”¨è®¢é˜…æº")
            return urls
    print(f"[âš ï¸] æ²¡æœ‰æ‰¾åˆ°å¤‡ç”¨è®¢é˜…æºæ–‡ä»¶")
    return []

def save_fallback_urls(urls):
    """ä¿å­˜ fallback URL åˆ—è¡¨åˆ°æœ¬åœ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(FALLBACK_FILE) or ".", exist_ok=True)
    with open(FALLBACK_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(urls))

async def fetch_subscription(session, url):
    """ä¸‹è½½è®¢é˜…å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for attempt in range(RETRY_COUNT):
        try:
            timeout = aiohttp.ClientTimeout(total=20 + attempt * 10)
            async with session.get(url, timeout=timeout, headers=headers) as resp:
                resp.raise_for_status()
                text = await resp.text()
                return url, text
        except Exception as e:
            if attempt == RETRY_COUNT - 1:
                print(f"[âŒ] ä¸‹è½½å¤±è´¥: {url} - {e}")
                return url, None
            else:
                print(f"[âš ï¸] å°è¯• {attempt + 1} å¤±è´¥ï¼Œé‡è¯•: {url}")
                await asyncio.sleep(3 * (attempt + 1))
    
    return url, None

def parse_clash_yaml(text):
    """è§£æ Clash YAML"""
    try:
        data = yaml.safe_load(text)
        if isinstance(data, dict) and "proxies" in data:
            valid_proxies = []
            for proxy in data["proxies"]:
                is_valid, reason = enhanced_validate_proxy(proxy)
                if is_valid:
                    valid_proxies.append(proxy)
                # ä¸å†æ‰“å°æ¯ä¸ªæ— æ•ˆé…ç½®çš„è¯¦ç»†ä¿¡æ¯
            return valid_proxies
    except Exception as e:
        print(f"[âš ï¸] è§£æ Clash YAML å¤±è´¥: {e}")
    return []

def parse_base64_links(text):
    """è§£æ Base64 è®¢é˜…"""
    proxies = []
    try:
        # å°è¯•ä¸åŒçš„è§£ç æ–¹å¼
        for decode_func in [
            lambda x: base64.b64decode(x + "==="),
            lambda x: base64.b64decode(x.replace('-', '+').replace('_', '/') + "==="),
            lambda x: base64.urlsafe_b64decode(x + "===")
        ]:
            try:
                decoded_text = decode_func(text.strip()).decode("utf-8", errors="ignore")
                break
            except:
                continue
        else:
            decoded_text = text.strip()
    except:
        decoded_text = text.strip()

    for line in decoded_text.splitlines():
        line = line.strip()
        if not line:
            continue
            
        if line.startswith("vless://"):
            proxy_config = parse_vless_url(line)
            if proxy_config:
                is_valid, reason = enhanced_validate_proxy(proxy_config)
                if is_valid:
                    proxies.append(proxy_config)
    
    return proxies

def parse_vless_url(url):
    """è§£æ vless URL"""
    try:
        url_part, *remark_part = url[8:].split("#", 1)
        base_name = urllib.parse.unquote(remark_part[0]) if remark_part else "vless"
        uuid, server_info = url_part.split("@", 1)
        server_port, *params_raw = server_info.split("?", 1)
        server, port = server_port.split(":", 1)
        params = urllib.parse.parse_qs(params_raw[0]) if params_raw else {}
        
        node_config = {
            "name": base_name,
            "type": "vless",
            "server": server,
            "port": int(port),
            "uuid": uuid,
            "network": params.get("type", ["tcp"])[0],
        }
        
        if node_config["network"] == "ws":
            path = params.get("path", [""])[0]
            ws_opts = {"path": path}
            if "host" in params:
                ws_opts["headers"] = {"Host": params["host"][0]}
            node_config["ws-opts"] = ws_opts
            
        if params.get("security", [""])[0] == "tls":
            node_config["tls"] = True
            if "sni" in params:
                node_config["servername"] = params["sni"][0]
        
        return node_config
    except Exception as e:
        return None

async def main():
    """ä¸»å‡½æ•° - å…¨é¢ä¼˜åŒ–æµç¨‹"""
    print("=== ä»£ç†èŠ‚ç‚¹è´¨é‡ä¼˜åŒ–å·¥å…· v2.0 ===")
    print("ä¼˜åŒ–ç‰¹æ€§:")
    print("âœ“ æ›´å®½æ¾çš„è´¨é‡æµ‹è¯•æ ‡å‡†")
    print("âœ“ ä¿ç•™åŸå§‹èŠ‚ç‚¹åç§°")
    print("âœ“ å¢å¼ºçš„è¿é€šæ€§æµ‹è¯•")
    print("âœ“ æ™ºèƒ½å»é‡ç®—æ³•")
    print()
    
    # åˆ†æå½“å‰èŠ‚ç‚¹(å¦‚æœå­˜åœ¨)
    if os.path.exists(OUTPUT_US):
        print("åˆ†æå½“å‰us.yamlæ–‡ä»¶...")
        analyze_current_nodes(OUTPUT_US)
        print()
    
    all_proxies = []

    print("--- è·å–è®¢é˜…æº ---")
    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(
            limit=30, 
            ttl_dns_cache=600,
            limit_per_host=5
        ),
        timeout=aiohttp.ClientTimeout(total=60)
    ) as session:
        subscription_urls = await fetch_subscription_urls(session)
        if not subscription_urls:
            print("[âŒ] æ— å¯ç”¨è®¢é˜… URL")
            return
        
        print("--- ä¸‹è½½è®¢é˜…å†…å®¹ ---")
        tasks = [fetch_subscription(session, url) for url in subscription_urls]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful_downloads = 0
        for result in responses:
            if isinstance(result, Exception):
                print(f"[âŒ] ä¸‹è½½å¼‚å¸¸: {result}")
                continue
                
            url, text = result
            if text:
                proxies = parse_clash_yaml(text) or parse_base64_links(text)
                if proxies:
                    print(f"[âœ…] {url} â†’ {len(proxies)} èŠ‚ç‚¹")
                    all_proxies.extend(proxies)
                    successful_downloads += 1
                else:
                    print(f"[âš ï¸] {url} â†’ æ— æœ‰æ•ˆèŠ‚ç‚¹")
            else:
                print(f"[âŒ] {url} â†’ ä¸‹è½½å¤±è´¥")
        
        print(f"\næˆåŠŸä¸‹è½½: {successful_downloads}/{len(subscription_urls)} ä¸ªè®¢é˜…æº")

    if not all_proxies:
        print("[âŒ] æœªè·å–åˆ°ä»»ä½•èŠ‚ç‚¹")
        # åˆ›å»ºç©ºæ–‡ä»¶é¿å…é”™è¯¯
        save_yaml_optimized(OUTPUT_ALL, [])
        save_yaml_optimized(OUTPUT_US, [])
        return

    print(f"\n--- èŠ‚ç‚¹å¤„ç†ä¸ç­›é€‰ ---")
    print(f"åŸå§‹èŠ‚ç‚¹æ•°: {len(all_proxies)}")
    
    # æ™ºèƒ½å»é‡
    deduplicated = intelligent_dedup(all_proxies)
    
    print(f"\n--- å¼€å§‹è´¨é‡æµ‹è¯• ---")
    print(f"æµ‹è¯•é…ç½®: è¶…æ—¶{TEST_TIMEOUT}s, å»¶è¿Ÿé˜ˆå€¼{LATENCY_THRESHOLD}ms, æˆåŠŸç‡é˜ˆå€¼{SUCCESS_RATE_THRESHOLD*100}%")
    
    tested_proxies = []
    failed_proxies = 0

    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(
            limit=MAX_CONCURRENCY,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        ),
        timeout=aiohttp.ClientTimeout(total=TEST_TIMEOUT * 2)
    ) as session:
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡è½½
        batch_size = MAX_CONCURRENCY
        total_batches = (len(deduplicated) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(deduplicated))
            batch = deduplicated[start_idx:end_idx]
            
            print(f"\n[ğŸ“Š] å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} ä¸ªèŠ‚ç‚¹)")
            
            tasks = [relaxed_quality_test(session, proxy) for proxy in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            batch_success = 0
            for result in results:
                if isinstance(result, Exception):
                    print(f"[âŒ] æµ‹è¯•å¼‚å¸¸: {result}")
                    failed_proxies += 1
                    continue
                
                tested_proxy, status = result
                if tested_proxy:
                    tested_proxies.append(tested_proxy)
                    batch_success += 1
                else:
                    failed_proxies += 1
            
            print(f"[âœ…] æ‰¹æ¬¡ {batch_idx + 1} æˆåŠŸ: {batch_success}/{len(batch)}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_idx < total_batches - 1:
                await asyncio.sleep(2)

    print(f"\n--- æµ‹è¯•ç»“æœæ€»ç»“ ---")
    print(f"æ€»å…±æµ‹è¯•èŠ‚ç‚¹: {len(deduplicated)}")
    print(f"æˆåŠŸèŠ‚ç‚¹æ•°: {len(tested_proxies)}")
    print(f"å¤±è´¥èŠ‚ç‚¹æ•°: {failed_proxies}")
    print(f"æˆåŠŸç‡: {len(tested_proxies)/len(deduplicated)*100:.1f}%")

    if not tested_proxies:
        print("[âŒ] æ²¡æœ‰èŠ‚ç‚¹é€šè¿‡æµ‹è¯•ï¼Œä¿å­˜ç©ºé…ç½®")
        save_yaml_optimized(OUTPUT_ALL, [])
        save_yaml_optimized(OUTPUT_US, [])
        return
    
    # ç­›é€‰USèŠ‚ç‚¹
    print(f"\n--- ç­›é€‰åœ°åŒºèŠ‚ç‚¹ ---")
    us_nodes = enhanced_us_filter(tested_proxies)
    
    # ä¿å­˜ç»“æœ
    print(f"\n--- ä¿å­˜ç»“æœ ---")
    save_yaml_optimized(OUTPUT_ALL, tested_proxies)
    save_yaml_optimized(OUTPUT_US, us_nodes)
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    generate_quality_report(tested_proxies, us_nodes)
    
    print(f"\n=== å®Œæˆ ===")
    print(f"âœ… å…¨é‡èŠ‚ç‚¹: {len(tested_proxies)} ä¸ª")
    print(f"âœ… USèŠ‚ç‚¹: {len(us_nodes)} ä¸ª")
    print(f"ğŸ“Š é…ç½®å·²ä¿å­˜åˆ° providers/ ç›®å½•")

def generate_quality_report(all_nodes, us_nodes):
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    try:
        if not all_nodes:
            return
            
        scores = [node.get('quality_score', 0) for node in all_nodes]
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "total_nodes": len(all_nodes),
            "us_nodes": len(us_nodes),
            "quality_stats": {
                "min_score": min(scores),
                "max_score": max(scores),
                "avg_score": statistics.mean(scores),
                "median_score": statistics.median(scores)
            },
            "grade_distribution": {
                "excellent": len([s for s in scores if s >= 80]),
                "good": len([s for s in scores if 60 <= s < 80]),
                "fair": len([s for s in scores if 40 <= s < 60]),
                "poor": len([s for s in scores if s < 40])
            },
            "top_nodes": [
                {
                    "name": node.get('name', 'Unknown'),
                    "server": node.get('server', 'Unknown'),
                    "score": node.get('quality_score', 0)
                }
                for node in sorted(all_nodes, key=lambda x: x.get('quality_score', 0), reverse=True)[:10]
            ]
        }
        
        with open(QUALITY_REPORT, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        print(f"[ğŸ“Š] è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ° {QUALITY_REPORT}")
        
    except Exception as e:
        print(f"[âš ï¸] ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")

# ä¸»å…¥å£
if __name__ == "__main__":
    try:
        # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆWindowså…¼å®¹ï¼‰
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        
        asyncio.run(main())
        
    except KeyboardInterrupt:
        print("\n[âš ï¸] ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n[âŒ] ç¨‹åºå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
