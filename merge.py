import requests
import yaml
import os
import base64
import json
from hashlib import md5
import sys
import time
import urllib.parse
import asyncio
import aiohttp
import socket
import concurrent.futures
from aiohttp import client_exceptions
import statistics
from datetime import datetime, timedelta
import random
import ipaddress
from collections import defaultdict

# ========== é…ç½®ï¼šå›ºå®šæ›´æ–°æ–‡ä»¶ URL å’Œæ–‡ä»¶è·¯å¾„ ==========
UPDATE_FILE_URL = "https://apicsv.sosorg.nyc.mn/gengxin.txt?token=CMorg"
FALLBACK_FILE = "fallback_urls.txt"
OUTPUT_ALL = "providers/all.yaml"
OUTPUT_US = "providers/us.yaml"
QUALITY_REPORT = "quality_report.json"
BLACKLIST_FILE = "blacklist_ips.txt"

# ä¼˜åŒ–çš„æµ‹è¯•é…ç½®
TEST_URLS = [
    "http://cp.cloudflare.com/generate_204",
    "http://www.google.com/generate_204",
    "http://detectportal.firefox.com/success.txt",
    "http://connectivity-check.ubuntu.com/"
]
TEST_TIMEOUT = 12  # è¿›ä¸€æ­¥é™ä½è¶…æ—¶ï¼Œå¿«é€Ÿæ·˜æ±°æ…¢èŠ‚ç‚¹
MAX_CONCURRENCY = 25  # é™ä½å¹¶å‘æ•°æé«˜ç¨³å®šæ€§
RETRY_COUNT = 2
LATENCY_THRESHOLD = 1000  # é™ä½å»¶è¿Ÿé˜ˆå€¼åˆ°1ç§’
SUCCESS_RATE_THRESHOLD = 0.7  # æé«˜æˆåŠŸç‡è¦æ±‚
MAX_NODES_PER_IP = 3  # é™åˆ¶æ¯ä¸ªIPçš„èŠ‚ç‚¹æ•°é‡
MIN_QUALITY_SCORE = 60  # æœ€ä½è´¨é‡åˆ†æ•°è¦æ±‚

# è´¨é‡è¯„åˆ†æƒé‡ - è°ƒæ•´æƒé‡æ›´æ³¨é‡ç¨³å®šæ€§
WEIGHTS = {
    'latency': 0.35,
    'success_rate': 0.35,
    'stability': 0.25,
    'diversity': 0.05
}

# ========== IP è´¨é‡ç®¡ç† ==========
def load_ip_blacklist():
    """åŠ è½½IPé»‘åå•"""
    if os.path.exists(BLACKLIST_FILE):
        with open(BLACKLIST_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_ip_blacklist(blacklist):
    """ä¿å­˜IPé»‘åå•"""
    with open(BLACKLIST_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(blacklist)))

def is_valid_ip(ip):
    """éªŒè¯IPåœ°å€æœ‰æ•ˆæ€§ï¼Œæ’é™¤å†…ç½‘IP"""
    try:
        ip_obj = ipaddress.ip_address(ip)
        # æ’é™¤å†…ç½‘ã€å›ç¯ã€å¤šæ’­ç­‰ç‰¹æ®ŠIP
        if (ip_obj.is_private or ip_obj.is_loopback or
            ip_obj.is_multicast or ip_obj.is_reserved):
            return False
        return True
    except:
        return False

def analyze_current_nodes(yaml_file):
    """åˆ†æå½“å‰èŠ‚ç‚¹è´¨é‡é—®é¢˜"""
    try:
        with open(yaml_file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            proxies = data.get('proxies', [])
        
        print(f"\n=== å½“å‰èŠ‚ç‚¹åˆ†æ ===")
        print(f"æ€»èŠ‚ç‚¹æ•°: {len(proxies)}")
        
        # ç»Ÿè®¡IPä½¿ç”¨æƒ…å†µ
        ip_count = defaultdict(int)
        uuid_count = defaultdict(int)
        name_patterns = defaultdict(int)
        
        for proxy in proxies:
            ip = proxy.get('server', '')
            uuid = proxy.get('uuid', '')
            name = proxy.get('name', '')
            
            ip_count[ip] += 1
            uuid_count[uuid] += 1
            # æå–åç§°æ¨¡å¼
            base_name = name.split('ã€')[0] if 'ã€' in name else name
            name_patterns[base_name] += 1
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"å”¯ä¸€IPæ•°é‡: {len(ip_count)}")
        print(f"å”¯ä¸€UUIDæ•°é‡: {len(uuid_count)}")
        
        # é‡å¤IPè¿‡å¤šçš„æƒ…å†µ
        high_repeat_ips = {ip: count for ip, count in ip_count.items() if count > 10}
        if high_repeat_ips:
            print(f"âš ï¸ é«˜é‡å¤IP ({len(high_repeat_ips)}ä¸ª):")
            for ip, count in sorted(high_repeat_ips.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  {ip}: {count}ä¸ªèŠ‚ç‚¹")
        
        # UUIDé‡å¤æƒ…å†µ
        if len(uuid_count) < 5:
            print(f"âš ï¸ UUIDå¤šæ ·æ€§ä¸è¶³ï¼Œåªæœ‰{len(uuid_count)}ä¸ªä¸åŒUUID")
        
        return proxies, ip_count, uuid_count
        
    except Exception as e:
        print(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
        return [], {}, {}

# ========== å¢å¼ºçš„èŠ‚ç‚¹éªŒè¯å’Œç­›é€‰ ==========
def enhanced_validate_proxy(proxy):
    """å¢å¼ºçš„ä»£ç†é…ç½®éªŒè¯"""
    required_fields = ['name', 'type', 'server', 'port', 'uuid']
    
    # åŸºç¡€å­—æ®µæ£€æŸ¥
    for field in required_fields:
        if field not in proxy or not proxy[field]:
            return False, f"ç¼ºå°‘å­—æ®µ: {field}"
    
    # ç«¯å£éªŒè¯
    try:
        port = int(proxy['port'])
        if port <= 0 or port > 65535:
            return False, f"ç«¯å£èŒƒå›´é”™è¯¯: {port}"
    except:
        return False, "ç«¯å£ä¸æ˜¯æ•°å­—"
    
    # IPåœ°å€éªŒè¯
    server = proxy.get('server', '')
    if not is_valid_ip(server):
        return False, f"æ— æ•ˆIP: {server}"
    
    # UUIDéªŒè¯ (åŸºç¡€é•¿åº¦æ£€æŸ¥)
    uuid = proxy.get('uuid', '')
    if len(uuid) < 30:  # UUIDåº”è¯¥è¶³å¤Ÿé•¿
        return False, f"UUIDè¿‡çŸ­: {uuid}"
    
    # åè®®ç‰¹å®šéªŒè¯
    if proxy.get('type') == 'vless':
        if proxy.get('network') == 'ws':
            ws_opts = proxy.get('ws-opts', {})
            if not ws_opts.get('path'):
                return False, "WebSocketç¼ºå°‘path"
    
    return True, "valid"

def intelligent_dedup(proxies):
    """æ™ºèƒ½å»é‡ï¼Œä¿ç•™è´¨é‡æ›´å¥½çš„èŠ‚ç‚¹"""
    # æŒ‰æœåŠ¡å™¨IPåˆ†ç»„
    ip_groups = defaultdict(list)
    for proxy in proxies:
        ip = proxy.get('server', '')
        ip_groups[ip].append(proxy)
    
    result = []
    blacklist = load_ip_blacklist()
    
    for ip, group in ip_groups.items():
        # è·³è¿‡é»‘åå•IP
        if ip in blacklist:
            print(f"[âš ï¸] è·³è¿‡é»‘åå•IP: {ip}")
            continue
            
        # é™åˆ¶æ¯ä¸ªIPçš„èŠ‚ç‚¹æ•°é‡
        if len(group) > MAX_NODES_PER_IP:
            print(f"[ğŸ“Š] IP {ip} æœ‰ {len(group)} ä¸ªèŠ‚ç‚¹ï¼Œé™åˆ¶ä¸º {MAX_NODES_PER_IP} ä¸ª")
            # æŒ‰ç«¯å£å’Œé…ç½®å¤šæ ·æ€§æ’åº
            group.sort(key=lambda x: (x.get('port', 0), x.get('servername', ''), x.get('uuid', '')))
            group = group[:MAX_NODES_PER_IP]
        
        result.extend(group)
    
    print(f"[DEBUG] æ™ºèƒ½å»é‡åèŠ‚ç‚¹æ•°: {len(result)} (åŸå§‹: {len(proxies)})")
    return result

def enhanced_us_filter(proxies):
    """å¢å¼ºçš„USèŠ‚ç‚¹ç­›é€‰ï¼Œæ›´ç²¾ç¡®è¯†åˆ«"""
    us_nodes = []
    
    # æ‰©å±•çš„æ’é™¤å…³é”®è¯
    exclude_keywords = [
        # äºšæ´²
        "HK", "HONG KONG", "é¦™æ¸¯", "æ¸¯", "HONGKONG",
        "SG", "SINGAPORE", "æ–°åŠ å¡", "ç‹®åŸ",
        "JP", "JAPAN", "æ—¥æœ¬", "ä¸œäº¬", "TOKYO", "OSAKA",
        "KR", "KOREA", "éŸ©å›½", "é¦–å°”", "SEOUL",
        "TW", "TAIWAN", "å°æ¹¾", "å°åŒ—", "TAIPEI",
        "CN", "CHINA", "ä¸­å›½", "å¤§é™†", "MAINLAND",
        "MY", "MALAYSIA", "é©¬æ¥è¥¿äºš",
        "TH", "THAILAND", "æ³°å›½",
        "VN", "VIETNAM", "è¶Šå—",
        "IN", "INDIA", "å°åº¦",
        # æ¬§æ´²
        "UK", "LONDON", "è‹±å›½", "ä¼¦æ•¦", "BRITAIN",
        "DE", "GERMANY", "å¾·å›½", "æ³•å…°å…‹ç¦", "FRANKFURT",
        "FR", "FRANCE", "æ³•å›½", "å·´é»", "PARIS",
        "NL", "NETHERLANDS", "è·å…°", "AMSTERDAM",
        "IT", "ITALY", "æ„å¤§åˆ©",
        "ES", "SPAIN", "è¥¿ç­ç‰™",
        "RU", "RUSSIA", "ä¿„ç½—æ–¯", "è«æ–¯ç§‘",
        "TR", "TURKEY", "åœŸè€³å…¶",
        # å…¶ä»–
        "CA", "CANADA", "åŠ æ‹¿å¤§", "TORONTO",
        "AU", "AUSTRALIA", "æ¾³å¤§åˆ©äºš",
        "BR", "BRAZIL", "å·´è¥¿",
    ]
    
    # ç¾å›½å…³é”®è¯ - æ›´å…¨é¢
    us_keywords = [
        "US", "USA", "ç¾å›½", "UNITED STATES", "AMERICA", "AMERICAN",
        # ä¸»è¦åŸå¸‚
        "LOS ANGELES", "NEW YORK", "CHICAGO", "DALLAS", "HOUSTON",
        "SAN FRANCISCO", "SEATTLE", "MIAMI", "DENVER", "ATLANTA",
        "BOSTON", "PHILADELPHIA", "PHOENIX", "SAN DIEGO", "SAN JOSE",
        "AUSTIN", "COLUMBUS", "FORT WORTH", "CHARLOTTE", "DETROIT",
        "EL PASO", "MEMPHIS", "BALTIMORE", "MILWAUKEE", "ALBUQUERQUE",
        # å·å
        "VIRGINIA", "CALIFORNIA", "TEXAS", "OREGON", "FLORIDA",
        "WASHINGTON", "NEVADA", "ARIZONA", "COLORADO", "GEORGIA",
        "ILLINOIS", "OHIO", "PENNSYLVANIA", "MICHIGAN", "TENNESSEE",
        # ç¼©å†™
        "LA", "NYC", "SF", "DC", "VA", "CA", "TX", "FL", "WA"
    ]
    
    for proxy in proxies:
        name = proxy.get("name", "").upper()
        server = proxy.get("server", "")
        
        # æ£€æŸ¥åç§°ä¸­çš„å…³é”®è¯
        has_us_keyword = any(keyword in name for keyword in us_keywords)
        has_exclude_keyword = any(exclude in name for exclude in exclude_keywords)
        
        # åŸºäºIPåœ°å€çš„åœ°ç†ä½ç½®æ¨æ–­(ç®€å•çš„IPæ®µåˆ¤æ–­)
        ip_seems_us = is_likely_us_ip(server)
        
        if (has_us_keyword and not has_exclude_keyword) or ip_seems_us:
            if not has_exclude_keyword:  # å³ä½¿IPåƒç¾å›½ï¼Œä¹Ÿè¦æ’é™¤æ˜ç¡®æ ‡è®°ä¸ºå…¶ä»–å›½å®¶çš„
                us_nodes.append(proxy)
            else:
                print(f"[âš ï¸] IPä¼¼ä¹æ˜¯ç¾å›½ä½†åç§°æ˜¾ç¤ºå…¶ä»–åœ°åŒº: {proxy['name']}")
        elif has_exclude_keyword:
            print(f"[âš ï¸] æ’é™¤é US èŠ‚ç‚¹: {proxy['name']}")
    
    print(f"[DEBUG] ç­›é€‰å‡º {len(us_nodes)} ä¸ª US èŠ‚ç‚¹")
    return us_nodes

def is_likely_us_ip(ip):
    """ç®€å•çš„IPåœ°ç†ä½ç½®æ¨æ–­ - åŸºäºå·²çŸ¥çš„ç¾å›½IPæ®µ"""
    try:
        ip_obj = ipaddress.ip_address(ip)
        ip_int = int(ip_obj)
        
        # ä¸€äº›å·²çŸ¥çš„ç¾å›½IPæ®µ (ç®€åŒ–ç‰ˆæœ¬)
        us_ranges = [
            (ipaddress.ip_address('8.8.8.0'), ipaddress.ip_address('8.8.8.255')),  # Google DNS
            (ipaddress.ip_address('1.1.1.0'), ipaddress.ip_address('1.1.1.255')),  # Cloudflare
            # å¯ä»¥æ·»åŠ æ›´å¤šå·²çŸ¥çš„ç¾å›½IPæ®µ
        ]
        
        for start, end in us_ranges:
            if int(start) <= ip_int <= int(end):
                return True
                
    except:
        pass
    return False

# ========== å¢å¼ºçš„è¿æ¥è´¨é‡æµ‹è¯• ==========
async def comprehensive_quality_test(session, proxy_config):
    """å…¨é¢çš„èŠ‚ç‚¹è´¨é‡æµ‹è¯•"""
    node_name = proxy_config.get('name', 'æœªçŸ¥èŠ‚ç‚¹')
    server = proxy_config.get('server')
    port = int(proxy_config.get('port', 0))
    
    if not server or not port:
        return None, "æ— æ•ˆé…ç½®"
    
    print(f"[ğŸ”] æµ‹è¯•èŠ‚ç‚¹: {node_name} ({server}:{port})")
    
    # 1. å¤šè½® Socket è¿æ¥æµ‹è¯•
    socket_results = []
    for round_num in range(3):
        latency = await test_socket_connection(server, port, timeout=8)
        if latency is not None:
            socket_results.append(latency)
        await asyncio.sleep(0.2)  # è½®æ¬¡é—´éš”
    
    if len(socket_results) < 2:  # è‡³å°‘æˆåŠŸ2æ¬¡
        print(f"[âŒ] {node_name} | Socket è¿æ¥å¤±è´¥")
        return None, "Socketè¿æ¥å¤±è´¥"
    
    socket_avg = statistics.mean(socket_results)
    socket_std = statistics.stdev(socket_results) if len(socket_results) > 1 else 0
    
    if socket_avg > LATENCY_THRESHOLD:
        print(f"[âŒ] {node_name} | å»¶è¿Ÿè¿‡é«˜: {socket_avg:.0f}ms")
        return None, f"å»¶è¿Ÿè¿‡é«˜: {socket_avg:.0f}ms"
    
    # 2. HTTP è¿é€šæ€§æµ‹è¯•
    http_results = []
    test_urls = random.sample(TEST_URLS, min(2, len(TEST_URLS)))
    
    for test_url in test_urls:
        for attempt in range(2):  # æ¯ä¸ªURLæµ‹è¯•2æ¬¡
            try:
                start_time = time.time()
                timeout = aiohttp.ClientTimeout(total=TEST_TIMEOUT)
                async with session.get(test_url, timeout=timeout) as resp:
                    await resp.read()
                    latency = (time.time() - start_time) * 1000
                    if resp.status in [200, 204]:
                        http_results.append(latency)
                        break
                    await asyncio.sleep(0.1)
            except Exception as e:
                if attempt == 1:  # æœ€åä¸€æ¬¡å°è¯•
                    print(f"[âš ï¸] {node_name} HTTPæµ‹è¯•å¤±è´¥: {test_url} - {e}")
                continue
    
    # 3. ç»¼åˆè¯„åˆ†
    all_latencies = socket_results + http_results
    total_tests = 6  # 3æ¬¡socket + æœ€å¤š4æ¬¡HTTP
    success_count = len(all_latencies)
    success_rate = success_count / total_tests
    
    if success_rate < SUCCESS_RATE_THRESHOLD:
        print(f"[âŒ] {node_name} | æˆåŠŸç‡è¿‡ä½: {success_rate:.2f}")
        return None, f"æˆåŠŸç‡è¿‡ä½: {success_rate:.2f}"
    
    # è®¡ç®—è´¨é‡åˆ†æ•°
    quality_score = calculate_enhanced_quality_score(
        all_latencies, success_count, total_tests, socket_std
    )
    
    if quality_score < MIN_QUALITY_SCORE:
        print(f"[âŒ] {node_name} | è´¨é‡åˆ†æ•°è¿‡ä½: {quality_score:.2f}")
        return None, f"è´¨é‡åˆ†æ•°è¿‡ä½: {quality_score:.2f}"
    
    # æˆåŠŸçš„èŠ‚ç‚¹
    proxy_config['quality_score'] = quality_score
    proxy_config['test_info'] = {
        'avg_latency': round(statistics.mean(all_latencies), 2),
        'socket_latency': round(socket_avg, 2),
        'socket_stability': round(socket_std, 2),
        'success_rate': round(success_rate, 3),
        'test_time': datetime.now().isoformat()
    }
    
    print(f"[âœ…] {node_name} | è´¨é‡: {quality_score:.2f} | å»¶è¿Ÿ: {socket_avg:.0f}Â±{socket_std:.0f}ms | æˆåŠŸç‡: {success_count}/{total_tests}")
    return proxy_config, "success"

def calculate_enhanced_quality_score(latencies, success_count, total_tests, stability_variance):
    """å¢å¼ºçš„è´¨é‡åˆ†æ•°è®¡ç®—"""
    if not latencies:
        return 0
    
    avg_latency = statistics.mean(latencies)
    success_rate = success_count / total_tests if total_tests > 0 else 0
    
    # å»¶è¿Ÿåˆ†æ•° (0-100)
    latency_score = max(0, 100 - (avg_latency / 15))
    
    # æˆåŠŸç‡åˆ†æ•° (0-100)
    success_score = success_rate * 100
    
    # ç¨³å®šæ€§åˆ†æ•° (åŸºäºå»¶è¿Ÿæ–¹å·®)
    stability_score = max(0, 100 - (stability_variance / 5))
    
    # é€Ÿåº¦åˆ†æ•° (åŸºäºæœ€å°å»¶è¿Ÿ)
    speed_score = max(0, 100 - (min(latencies) / 10)) if latencies else 0
    
    # ç»¼åˆè¯„åˆ†
    final_score = (
        latency_score * WEIGHTS['latency'] +
        success_score * WEIGHTS['success_rate'] +
        stability_score * WEIGHTS['stability'] +
        speed_score * 0.1  # é€Ÿåº¦æƒé‡
    )
    
    return round(final_score, 2)

async def test_socket_connection(server, port, timeout=8):
    """ä¼˜åŒ–çš„å¼‚æ­¥ Socket è¿æ¥æµ‹è¯•"""
    try:
        start_time = time.time()
        _, writer = await asyncio.wait_for(
            asyncio.open_connection(server, port),
            timeout=timeout
        )
        end_time = time.time()
        writer.close()
        await writer.wait_closed()
        return (end_time - start_time) * 1000
    except:
        return None

def save_yaml_optimized(path, proxies):
    """ä¼˜åŒ–çš„YAMLä¿å­˜ï¼Œç¡®ä¿Clashå…¼å®¹æ€§ï¼Œå¹¶è§£å†³èŠ‚ç‚¹åç§°é‡å¤é—®é¢˜"""
    abs_path = os.path.abspath(path)
    os.makedirs(os.path.dirname(abs_path), exist_ok=True)
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    sorted_proxies = sorted(proxies, key=lambda x: x.get('quality_score', 0), reverse=True)
    
    # æ¸…ç†é…ç½®ï¼Œç§»é™¤æµ‹è¯•æ•°æ®å¹¶ç¡®ä¿åç§°å”¯ä¸€
    clean_proxies = []
    name_counts = defaultdict(int)
    
    for proxy in sorted_proxies:
        clean_proxy = {k: v for k, v in proxy.items()
                       if k not in ['quality_score', 'test_info']}
                       
        # ç¡®ä¿åç§°å”¯ä¸€
        original_name = clean_proxy.get('name', 'unnamed')
        name_counts[original_name] += 1
        if name_counts[original_name] > 1:
            clean_proxy['name'] = f"{original_name} #{name_counts[original_name]}"
        
        clean_proxies.append(clean_proxy)
    
    # æ ‡å‡†Clashæ ¼å¼
    output_data = {"proxies": clean_proxies}
    
    with open(abs_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(output_data, f, allow_unicode=True, default_flow_style=False)
    
    print(f"[ğŸ’¾] å·²ä¿å­˜åˆ° {abs_path}ï¼ŒèŠ‚ç‚¹æ•°: {len(clean_proxies)}")
    
    if sorted_proxies and 'quality_score' in sorted_proxies[0]:
        avg_score = statistics.mean([p.get('quality_score', 0) for p in sorted_proxies])
        best_score = sorted_proxies[0]['quality_score']
        print(f"[ğŸ“Š] æœ€é«˜è´¨é‡åˆ†æ•°: {best_score:.2f}")
        print(f"[ğŸ“Š] å¹³å‡è´¨é‡åˆ†æ•°: {avg_score:.2f}")
        print(f"[â„¹ï¸] å·²ç§»é™¤æµ‹è¯•æ•°æ®ï¼Œç¡®ä¿ Clash å…¼å®¹æ€§")
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€ä½³èŠ‚ç‚¹
        print(f"[ğŸ†] å‰5ä¸ªæœ€ä½³èŠ‚ç‚¹:")
        for i, proxy in enumerate(sorted_proxies[:5], 1):
            print(f"  {i}. {proxy['name']} (åˆ†æ•°: {proxy['quality_score']:.2f})")

# ========== ä»å›ºå®š URL è·å–è®¢é˜…æº ==========
async def fetch_subscription_urls(session):
    """ä»å›ºå®š URL ä¸‹è½½è®¢é˜…æºåˆ—è¡¨"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        async with session.get(UPDATE_FILE_URL, timeout=15, headers=headers) as resp:
            resp.raise_for_status()
            content = await resp.text()
            if not content.strip():
                return load_fallback_urls()
            urls = [line.strip() for line in content.splitlines()
                   if line.strip() and line.strip().startswith('http')]
            if urls:
                print(f"[âœ…] è·å– {len(urls)} ä¸ªè®¢é˜…æº")
                save_fallback_urls(urls)
                return urls
            else:
                return load_fallback_urls()
    except Exception as e:
        print(f"[âŒ] è·å–è®¢é˜…æºå¤±è´¥: {e}")
        return load_fallback_urls()

def load_fallback_urls():
    """åŠ è½½æœ¬åœ°ä¿å­˜çš„ fallback URL åˆ—è¡¨"""
    if os.path.exists(FALLBACK_FILE):
        with open(FALLBACK_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
    return []

def save_fallback_urls(urls):
    """ä¿å­˜ fallback URL åˆ—è¡¨åˆ°æœ¬åœ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(FALLBACK_FILE) or ".", exist_ok=True)
    with open(FALLBACK_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(urls))

async def fetch_subscription(session, url):
    """ä¸‹è½½è®¢é˜…å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for attempt in range(RETRY_COUNT):
        try:
            timeout = aiohttp.ClientTimeout(total=15 + attempt * 5)
            async with session.get(url, timeout=timeout, headers=headers) as resp:
                resp.raise_for_status()
                text = await resp.text()
                return url, text
        except Exception as e:
            if attempt == RETRY_COUNT - 1:
                print(f"[âŒ] ä¸‹è½½å¤±è´¥: {url} - {e}")
                return url, None
            else:
                await asyncio.sleep(2 ** attempt)
    
    return url, None

def parse_clash_yaml(text):
    """è§£æ Clash YAML"""
    try:
        data = yaml.safe_load(text)
        if isinstance(data, dict) and "proxies" in data:
            valid_proxies = []
            for proxy in data["proxies"]:
                is_valid, reason = enhanced_validate_proxy(proxy)
                if is_valid:
                    valid_proxies.append(proxy)
                else:
                    print(f"[âš ï¸] è·³è¿‡æ— æ•ˆé…ç½®: {proxy.get('name', 'unknown')} - {reason}")
            return valid_proxies
    except Exception as e:
        print(f"[âš ï¸] è§£æ Clash YAML å¤±è´¥: {e}")
    return []

def parse_base64_links(text):
    """è§£æ Base64 è®¢é˜…"""
    proxies = []
    try:
        # å°è¯•ä¸åŒçš„è§£ç æ–¹å¼
        for decode_func in [
            lambda x: base64.b64decode(x + "==="),
            lambda x: base64.b64decode(x.replace('-', '+').replace('_', '/') + "==="),
            lambda x: base64.urlsafe_b64decode(x + "===")
        ]:
            try:
                decoded_text = decode_func(text.strip()).decode("utf-8", errors="ignore")
                break
            except:
                continue
        else:
            decoded_text = text.strip()
    except:
        decoded_text = text.strip()

    for line in decoded_text.splitlines():
        line = line.strip()
        if not line:
            continue
            
        if line.startswith("vless://"):
            proxy_config = parse_vless_url(line)
            if proxy_config:
                is_valid, reason = enhanced_validate_proxy(proxy_config)
                if is_valid:
                    proxies.append(proxy_config)
                else:
                    print(f"[âš ï¸] è·³è¿‡æ— æ•ˆvlessé…ç½®: {reason}")
    
    return proxies

def parse_vless_url(url):
    """è§£æ vless URL"""
    try:
        url_part, *remark_part = url[8:].split("#", 1)
        base_name = urllib.parse.unquote(remark_part[0]) if remark_part else "vless"
        uuid, server_info = url_part.split("@", 1)
        server_port, *params_raw = server_info.split("?", 1)
        server, port = server_port.split(":", 1)
        params = urllib.parse.parse_qs(params_raw[0]) if params_raw else {}
        
        node_config = {
            "name": base_name,
            "type": "vless",
            "server": server,
            "port": int(port),
            "uuid": uuid,
            "network": params.get("type", ["tcp"])[0],
        }
        
        if node_config["network"] == "ws":
            path = params.get("path", [""])[0]
            ws_opts = {"path": path}
            if "host" in params:
                ws_opts["headers"] = {"Host": params["host"][0]}
            node_config["ws-opts"] = ws_opts
            
        if params.get("security", [""])[0] == "tls":
            node_config["tls"] = True
            if "sni" in params:
                node_config["servername"] = params["sni"][0]
        
        return node_config
    except Exception as e:
        return None

async def main():
    """ä¸»å‡½æ•° - å…¨é¢ä¼˜åŒ–æµç¨‹"""
    print("=== ä»£ç†èŠ‚ç‚¹è´¨é‡ä¼˜åŒ–å·¥å…· ===\n")
    
    # åˆ†æå½“å‰èŠ‚ç‚¹(å¦‚æœå­˜åœ¨)
    if os.path.exists(OUTPUT_US):
        print("åˆ†æå½“å‰us.yamlæ–‡ä»¶...")
        analyze_current_nodes(OUTPUT_US)
        print()
    
    all_proxies = []

    print("--- è·å–è®¢é˜…æº ---")
    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=50, ttl_dns_cache=300),
        timeout=aiohttp.ClientTimeout(total=30)
    ) as session:
        subscription_urls = await fetch_subscription_urls(session)
        if not subscription_urls:
            print("[âŒ] æ— å¯ç”¨è®¢é˜… URL")
            return
        
        print("--- ä¸‹è½½è®¢é˜…å†…å®¹ ---")
        tasks = [fetch_subscription(session, url) for url in subscription_urls]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in responses:
            if isinstance(result, Exception):
                continue
                
            url, text = result
            if text:
                proxies = parse_clash_yaml(text) or parse_base64_links(text)
                if proxies:
                    print(f"[âœ…] {url} â†’ {len(proxies)} èŠ‚ç‚¹")
                    all_proxies.extend(proxies)

    if not all_proxies:
        print("[âŒ] æœªè·å–åˆ°ä»»ä½•èŠ‚ç‚¹")
        return

    print(f"\n--- èŠ‚ç‚¹å¤„ç†ä¸ç­›é€‰ ---")
    print(f"åŸå§‹èŠ‚ç‚¹æ•°: {len(all_proxies)}")
    
    # æ™ºèƒ½å»é‡
    deduplicated = intelligent_dedup(all_proxies)
    
    print("\n--- å¼€å§‹å…¨é¢è´¨é‡æµ‹è¯• ---")
    tested_proxies = []
    failed_proxies = 0

    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=MAX_CONCURRENCY),
        timeout=aiohttp.ClientTimeout(total=TEST_TIMEOUT * 2) # æµ‹è¯•è¶…æ—¶æ—¶é—´å¯ä»¥æ›´é•¿
    ) as session:
        tasks = [
            comprehensive_quality_test(session, proxy)
            for proxy in deduplicated
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, Exception):
                print(f"[âŒ] æµ‹è¯•ä¸­å‘ç”Ÿå¼‚å¸¸: {result}")
                continue
            
            tested_proxy, status = result
            if tested_proxy:
                tested_proxies.append(tested_proxy)
            else:
                failed_proxies += 1

    print(f"\n--- æµ‹è¯•ç»“æœæ€»ç»“ ---")
    print(f"æ€»å…±æµ‹è¯•èŠ‚ç‚¹: {len(deduplicated)}")
    print(f"æˆåŠŸèŠ‚ç‚¹æ•°: {len(tested_proxies)}")
    print(f"å¤±è´¥èŠ‚ç‚¹æ•°: {failed_proxies}")

    # ä¿å­˜ US èŠ‚ç‚¹
    us_nodes = enhanced_us_filter(tested_proxies)
    save_yaml_optimized(OUTPUT_US, us_nodes)
    
    # ä¿å­˜æ‰€æœ‰é€šè¿‡æµ‹è¯•çš„èŠ‚ç‚¹
    save_yaml_optimized(OUTPUT_ALL, tested_proxies)
    
# ä¸»å…¥å£
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
