import requests
import yaml
import os
import base64
import json
from hashlib import md5
import sys
import time
import urllib.parse
import asyncio
import aiohttp
import socket
import concurrent.futures
from aiohttp import client_exceptions

# ========== é…ç½®ï¼šå›ºå®šæ›´æ–°æ–‡ä»¶ URL å’Œæ–‡ä»¶è·¯å¾„ ==========
UPDATE_FILE_URL = "https://apicsv.sosorg.nyc.mn/gengxin.txt?token=CMorg"
FALLBACK_FILE = "fallback_urls.txt"
OUTPUT_ALL = "providers/all.yaml"
OUTPUT_US = "providers/us.yaml"

# æµ‹è¯•é…ç½®
TEST_URL = "http://cp.cloudflare.com/generate_204"
TEST_TIMEOUT = 20  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥æé«˜æˆåŠŸç‡
MAX_CONCURRENCY = 50  # å¹¶å‘æ•°

# ========== ç®¡ç† fallback URLs ==========
def load_fallback_urls():
    """åŠ è½½æœ¬åœ°ä¿å­˜çš„ fallback URL åˆ—è¡¨"""
    if os.path.exists(FALLBACK_FILE):
        with open(FALLBACK_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
    return []

def save_fallback_urls(urls):
    """ä¿å­˜ fallback URL åˆ—è¡¨åˆ°æœ¬åœ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(FALLBACK_FILE) or ".", exist_ok=True)
    with open(FALLBACK_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(urls))
    print(f"[âœ…] å·²ä¿å­˜ {len(urls)} ä¸ª URL åˆ° {FALLBACK_FILE}")

# ========== æ–°å¢ï¼šä»å›ºå®š URL è·å–è®¢é˜…æº ==========
async def fetch_subscription_urls(session):
    """ä»å›ºå®š URL ä¸‹è½½è®¢é˜…æºåˆ—è¡¨ï¼Œæ›´æ–°å¹¶è¿”å› fallback URLs"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
    }
    try:
        async with session.get(UPDATE_FILE_URL, timeout=15, headers=headers) as resp:
            resp.raise_for_status()
            content = await resp.text()
            print(f"[DEBUG] åŸå§‹å†…å®¹: {content[:100]}...")  # è°ƒè¯•å‰100å­—ç¬¦
            if not content.strip():
                print(f"[âš ï¸] {UPDATE_FILE_URL} æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
                return load_fallback_urls()
            urls = [line.strip() for line in content.splitlines() if line.strip() and line.strip().startswith('http')]
            if urls:
                print(f"[âœ…] ä» {UPDATE_FILE_URL} è·å– {len(urls)} ä¸ªè®¢é˜…æº")
                save_fallback_urls(urls)
                return urls
            else:
                print(f"[âš ï¸] {UPDATE_FILE_URL} æ— æœ‰æ•ˆ URLï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
                return load_fallback_urls()
    except Exception as e:
        print(f"[âŒ] ä¸‹è½½ {UPDATE_FILE_URL} å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
        return load_fallback_urls()

# ========== ä»£ç†å¤„ç†å‡½æ•° ==========
async def fetch_subscription(session, url):
    """å¼‚æ­¥ä¸‹è½½è®¢é˜…å†…å®¹ï¼Œè®¾ç½®è¶…æ—¶å’ŒçŠ¶æ€ç æ£€æŸ¥"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
    }
    try:
        async with session.get(url, timeout=15, headers=headers) as resp:
            resp.raise_for_status()
            text = await resp.text()
            print(f"[DEBUG] è®¢é˜… {url} å†…å®¹é¦–100å­—ç¬¦: {text[:100]}...")  # è°ƒè¯•è®¢é˜…å†…å®¹
            return url, text
    except client_exceptions.ClientError as e:
        print(f"[âŒ] ä¸‹è½½å¤±è´¥: {url} é”™è¯¯: {e}", file=sys.stderr)
        return url, None
    except asyncio.TimeoutError:
        print(f"[âŒ] ä¸‹è½½è¶…æ—¶: {url}", file=sys.stderr)
        return url, None

def parse_clash_yaml(text):
    """è§£æ Clash YAML æ ¼å¼çš„è®¢é˜…"""
    try:
        data = yaml.safe_load(text)
        if isinstance(data, dict) and "proxies" in data:
            print(f"[DEBUG] è§£æåˆ° {len(data['proxies'])} ä¸ª Clash èŠ‚ç‚¹")
            return data["proxies"]
    except Exception as e:
        print(f"[âš ï¸] è§£æ Clash YAML å¤±è´¥: {e}ï¼Œå†…å®¹: {text[:200]}...", file=sys.stderr)
    return []

def parse_base64_links(text):
    """è§£æ Base64 ç¼–ç çš„è®¢é˜…é“¾æ¥ï¼Œä¸“æ³¨äº vless åè®®ï¼Œä½¿ç”¨åŸå§‹åç§°"""
    proxies = []
    uuid_count = {}
    seen_names = set()
    try:
        text_corrected = text.strip().replace('-', '+').replace('_', '/')
        decoded_text = base64.b64decode(text_corrected + "===").decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"[âš ï¸] Base64 è§£ç å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬", file=sys.stderr)
        decoded_text = text.strip()

    for line in decoded_text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            if line.startswith("vless://"):
                url_part, *remark_part = line[8:].split("#", 1)
                base_name = urllib.parse.unquote(remark_part[0]) if remark_part else "vless"
                uuid, server_info = url_part.split("@", 1)
                server_port, *params_raw = server_info.split("?", 1)
                server, port = server_port.split(":", 1)
                params = urllib.parse.parse_qs(params_raw[0]) if params_raw else {}
                
                name = base_name
                if name in seen_names:
                    name = f"{base_name}_{server}_{port}"
                seen_names.add(name)
                
                uuid_count[uuid] = uuid_count.get(uuid, 0) + 1
                if uuid_count[uuid] > 5:
                    print(f"[âš ï¸] UUID {uuid} é‡å¤ä½¿ç”¨è¶…è¿‡ 5 æ¬¡ï¼Œå¯èƒ½å½±å“èŠ‚ç‚¹å¯ç”¨æ€§", file=sys.stderr)
                
                node_config = {
                    "name": name,
                    "type": "vless",
                    "server": server,
                    "port": int(port),
                    "uuid": uuid,
                    "network": params.get("type", ["tcp"])[0],
                }
                if node_config["network"] == "ws":
                    path = params.get("path", [""])[0]
                    if "proxyip:port(443)" in path:
                        path = path.replace("proxyip:port(443)", f"{server}:{port}")
                    ws_opts = {"path": path}
                    if "host" in params:
                        ws_opts["headers"] = {"Host": params["host"][0]}
                    node_config["ws-opts"] = ws_opts
                if params.get("security", [""])[0] == "tls":
                    node_config["tls"] = True
                    if "sni" in params:
                        node_config["servername"] = params["sni"][0]
                
                proxies.append(node_config)
        except Exception as e:
            print(f"[âš ï¸] è§£æèŠ‚ç‚¹é“¾æ¥å¤±è´¥: {line[:50]}... é”™è¯¯: {e}", file=sys.stderr)
    print(f"[DEBUG] è§£æåˆ° {len(proxies)} ä¸ª vless èŠ‚ç‚¹")
    return proxies

def deduplicate(proxies):
    """ä½¿ç”¨ md5 å¯¹èŠ‚ç‚¹è¿›è¡Œå»é‡ï¼Œé’ˆå¯¹ vless ä¼˜åŒ–é”®"""
    seen = set()
    result = []
    for p in proxies:
        key_parts = [p.get('server', ''), str(p.get('port', 0)), p.get('type', ''), p.get('uuid', '')]
        if 'ws-opts' in p and p['ws-opts'].get('path'):
            key_parts.append(p['ws-opts']['path'])
        key = md5(':'.join(key_parts).encode()).hexdigest()
        if key not in seen:
            seen.add(key)
            result.append(p)
    print(f"[DEBUG] å»é‡åèŠ‚ç‚¹æ•°: {len(result)}")
    return result

def filter_us(proxies):
    """æ”¾å®½ç­›é€‰æ¡ä»¶ï¼Œæ•è· US èŠ‚ç‚¹ï¼Œæ’é™¤é US èŠ‚ç‚¹"""
    us_nodes = []
    exclude_keywords = ["HK", "HONG KONG", "é¦™æ¸¯", "SG", "SINGAPORE", "æ–°åŠ å¡", "JP", "JAPAN", "æ—¥æœ¬"]
    for p in proxies:
        name = p.get("name", "").upper()
        if any(keyword in name for keyword in ["US", "USA", "ç¾å›½", "UNITED STATES", "AMERICA"]):
            if not any(exclude in name for exclude in exclude_keywords):
                us_nodes.append(p)
            else:
                print(f"[âš ï¸] æ’é™¤é US èŠ‚ç‚¹: {p['name']}", file=sys.stderr)
    print(f"[DEBUG] ç­›é€‰å‡º {len(us_nodes)} ä¸ª US èŠ‚ç‚¹")
    return us_nodes

def save_yaml(path, proxies):
    """å°†ä»£ç†åˆ—è¡¨ä¿å­˜ä¸º YAML æ–‡ä»¶"""
    import os
    abs_path = os.path.abspath(path)
    os.makedirs(os.path.dirname(abs_path), exist_ok=True)
    with open(abs_path, "w", encoding="utf-8") as f:
        yaml.safe_dump({"proxies": proxies}, f, allow_unicode=True, default_flow_style=False)
    print(f"[ğŸ’¾] å·²ä¿å­˜åˆ° {abs_path}ï¼ŒèŠ‚ç‚¹æ•°: {len(proxies)}")
    if os.path.exists(abs_path):
        with open(abs_path, "r", encoding="utf-8") as f:
            content = f.read()
            print(f"[DEBUG] æ–‡ä»¶ {abs_path} å†…å®¹é¦–è¡Œ: {content.splitlines()[0][:50]}...")
        print(f"[âœ…] æ–‡ä»¶ {abs_path} å­˜åœ¨")
    else:
        print(f"[âŒ] æ–‡ä»¶ {abs_path} æœªç”Ÿæˆ")

def direct_socket_test(server, port, timeout=TEST_TIMEOUT):
    """ç›´æ¥ä½¿ç”¨ socket æµ‹è¯• TCP è¿æ¥ï¼Œè¿”å›å»¶è¿Ÿ(ms)æˆ– None"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        start_time = time.time()
        result = sock.connect_ex((server, int(port)))
        end_time = time.time()
        sock.close()
        if result == 0:
            return (end_time - start_time) * 1000
    except Exception as e:
        print(f"[âš ï¸] Socket æµ‹è¯•å¤±è´¥: {server}:{port}, é”™è¯¯: {e}", file=sys.stderr)
    return None

async def test_connection_async(session, proxy_config, semaphore):
    """å¼‚æ­¥æµ‹è¯•å•ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ€§ï¼Œé’ˆå¯¹ vless ä¼˜åŒ–ï¼šsocket æµ‹è¯• + å»¶è¿Ÿè¿‡æ»¤"""
    async with semaphore:
        node_name = proxy_config.get('name', 'æœªçŸ¥èŠ‚ç‚¹')
        server = proxy_config.get('server')
        port = int(proxy_config.get('port', 0))
        if not server or not port:
            print(f"[âŒ] {node_name} | ç¼ºå°‘æœåŠ¡å™¨æˆ–ç«¯å£ä¿¡æ¯", file=sys.stderr)
            return None

        loop = asyncio.get_running_loop()
        socket_latency = await loop.run_in_executor(
            concurrent.futures.ThreadPoolExecutor(),
            direct_socket_test, server, port
        )
        if socket_latency is None or socket_latency > 2000:
            print(f"[âŒ] {node_name} | Socket è¿æ¥å¤±è´¥æˆ–å»¶è¿Ÿè¿‡é«˜ ({socket_latency}ms)", file=sys.stderr)
            return None

        print(f"[âœ…] {node_name} | vless (Socket: {socket_latency:.0f}ms)")
        return proxy_config

async def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«å¼‚æ­¥ä¸‹è½½å’Œæµ‹è¯•æµç¨‹"""
    all_proxies = []

    print("--- å¼€å§‹ä»å›ºå®š URL è·å–è®¢é˜…æº ---")
    async with aiohttp.ClientSession() as session:
        subscription_urls = await fetch_subscription_urls(session)
        if not subscription_urls:
            print("[âŒ] æ— å¯ç”¨è®¢é˜… URLï¼Œé€€å‡º", file=sys.stderr)
            return
        
        print("--- å¼€å§‹ä¸‹è½½å¹¶åˆå¹¶è®¢é˜… ---")
        tasks = [fetch_subscription(session, url) for url in subscription_urls]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        for url, text in responses:
            if isinstance(text, Exception):
                print(f"[âŒ] ä»»åŠ¡å¤±è´¥: {url}ï¼Œé”™è¯¯: {text}", file=sys.stderr)
                continue
            if text:
                proxies = parse_clash_yaml(text) or parse_base64_links(text)
                if proxies:
                    print(f"[âœ…] è®¢é˜…: {url} â†’ {len(proxies)} èŠ‚ç‚¹")
                    all_proxies.extend(proxies)
                else:
                    print(f"[âš ï¸] æœªèƒ½è¯†åˆ«è®¢é˜…æ ¼å¼: {url}ï¼Œå†…å®¹: {text[:200]}...", file=sys.stderr)
            else:
                print(f"[âŒ] è·³è¿‡è®¢é˜…: {url}ï¼Œæ— å†…å®¹", file=sys.stderr)

    if not all_proxies:
        print("[âŒ] æœªè§£æåˆ°ä»»ä½•èŠ‚ç‚¹ï¼Œall.yaml å°†ä¸ºç©º", file=sys.stderr)
        save_yaml(OUTPUT_ALL, [])
        return

    merged = deduplicate(all_proxies)
    print(f"[ğŸ“¦] åˆå¹¶å¹¶å»é‡åèŠ‚ç‚¹æ€»æ•°: {len(merged)}")
    print(f"[ğŸ”] æ‰€æœ‰èŠ‚ç‚¹: {[p['name'] for p in merged]}")
    save_yaml(OUTPUT_ALL, merged)

    us_nodes_to_test = filter_us(merged)
    if not us_nodes_to_test:
        print("[âš ï¸] æœªæ‰¾åˆ°ä»»ä½• US èŠ‚ç‚¹ï¼Œus.yaml å°†ä¸ºç©º")
        save_yaml(OUTPUT_US, [])
        return

    available_us_nodes = []
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    async with aiohttp.ClientSession() as session:
        tasks = [test_connection_async(session, node, semaphore) for node in us_nodes_to_test]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    for result in results:
        if isinstance(result, Exception):
            print(f"[âš ï¸] èŠ‚ç‚¹æµ‹è¯•å¤±è´¥: {result}", file=sys.stderr)
            continue
        if result:
            available_us_nodes.append(result)

    available_us_nodes.sort(key=lambda x: x['name'])
    print(f"[âœ…] ç»è¿‡æµ‹è¯•ï¼Œè·å¾— {len(available_us_nodes)} ä¸ªå¯ç”¨ US èŠ‚ç‚¹")
    print(f"[ğŸ”] å¯ç”¨ US èŠ‚ç‚¹: {[node['name'] for node in available_us_nodes]}")
    
    if not available_us_nodes:
        print("[âš ï¸] æ‰€æœ‰ US èŠ‚ç‚¹æµ‹è¯•å¤±è´¥ï¼Œus.yaml å°†ä¸ºç©º")
        save_yaml(OUTPUT_US, [])
    else:
        save_yaml(OUTPUT_US, available_us_nodes)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nè„šæœ¬å·²æ‰‹åŠ¨åœæ­¢ã€‚")
    except Exception as e:
        print(f"è„šæœ¬è¿è¡Œå‡ºé”™: {e}", file=sys.stderr)
