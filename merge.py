import requests
import yaml
import os
import base64
import json
from hashlib import md5
import sys
import time
import urllib.parse
import asyncio
import aiohttp
import socket
import concurrent.futures
from aiohttp import client_exceptions
import statistics
from datetime import datetime, timedelta
import random

# ========== é…ç½®ï¼šå›ºå®šæ›´æ–°æ–‡ä»¶ URL å’Œæ–‡ä»¶è·¯å¾„ ==========
UPDATE_FILE_URL = "https://apicsv.sosorg.nyc.mn/gengxin.txt?token=CMorg"
FALLBACK_FILE = "fallback_urls.txt"
OUTPUT_ALL = "providers/all.yaml"
OUTPUT_US = "providers/us.yaml"
QUALITY_REPORT = "quality_report.json"

# æµ‹è¯•é…ç½® - ä¼˜åŒ–çš„æµ‹è¯•å‚æ•°
TEST_URLS = [
    "http://cp.cloudflare.com/generate_204",
    "http://www.google.com/generate_204", 
    "http://detectportal.firefox.com/success.txt",
    "http://connectivity-check.ubuntu.com/"
]
TEST_TIMEOUT = 15  # é™ä½è¶…æ—¶æ—¶é—´ï¼Œå¿«é€Ÿæ·˜æ±°æ…¢èŠ‚ç‚¹
MAX_CONCURRENCY = 30  # é™ä½å¹¶å‘æ•°ï¼Œæé«˜ç¨³å®šæ€§
RETRY_COUNT = 2  # é‡è¯•æ¬¡æ•°
LATENCY_THRESHOLD = 1500  # å»¶è¿Ÿé˜ˆå€¼(ms)
SUCCESS_RATE_THRESHOLD = 0.6  # æˆåŠŸç‡é˜ˆå€¼

# è´¨é‡è¯„åˆ†æƒé‡
WEIGHTS = {
    'latency': 0.4,      # å»¶è¿Ÿæƒé‡
    'success_rate': 0.3,  # æˆåŠŸç‡æƒé‡
    'stability': 0.2,     # ç¨³å®šæ€§æƒé‡
    'speed': 0.1         # é€Ÿåº¦æƒé‡
}

# ========== ç®¡ç† fallback URLs ==========
def load_fallback_urls():
    """åŠ è½½æœ¬åœ°ä¿å­˜çš„ fallback URL åˆ—è¡¨"""
    if os.path.exists(FALLBACK_FILE):
        with open(FALLBACK_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
    return []

def save_fallback_urls(urls):
    """ä¿å­˜ fallback URL åˆ—è¡¨åˆ°æœ¬åœ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(FALLBACK_FILE) or ".", exist_ok=True)
    with open(FALLBACK_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(urls))
    print(f"[âœ…] å·²ä¿å­˜ {len(urls)} ä¸ª URL åˆ° {FALLBACK_FILE}")

# ========== å¢å¼ºçš„è´¨é‡æ£€æµ‹åŠŸèƒ½ ==========
def load_quality_history():
    """åŠ è½½å†å²è´¨é‡æ•°æ®"""
    if os.path.exists(QUALITY_REPORT):
        try:
            with open(QUALITY_REPORT, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_quality_history(quality_data):
    """ä¿å­˜è´¨é‡æ•°æ®"""
    with open(QUALITY_REPORT, "w", encoding="utf-8") as f:
        json.dump(quality_data, f, indent=2, ensure_ascii=False)

def calculate_quality_score(latencies, success_count, total_tests, historical_data=None):
    """è®¡ç®—èŠ‚ç‚¹è´¨é‡åˆ†æ•°"""
    if not latencies:
        return 0
    
    # å»¶è¿Ÿåˆ†æ•° (è¶Šä½è¶Šå¥½)
    avg_latency = statistics.mean(latencies)
    latency_score = max(0, 100 - (avg_latency / 20))  # 2000ms = 0åˆ†
    
    # æˆåŠŸç‡åˆ†æ•°
    success_rate = success_count / total_tests if total_tests > 0 else 0
    success_score = success_rate * 100
    
    # ç¨³å®šæ€§åˆ†æ•° (å»¶è¿Ÿæ–¹å·®è¶Šå°è¶Šå¥½)
    stability_score = 100
    if len(latencies) > 1:
        latency_std = statistics.stdev(latencies)
        stability_score = max(0, 100 - (latency_std / 10))
    
    # é€Ÿåº¦åˆ†æ•° (åŸºäºæœ€å°å»¶è¿Ÿ)
    speed_score = max(0, 100 - (min(latencies) / 15)) if latencies else 0
    
    # å†å²è¡¨ç°åŠ æƒ
    historical_bonus = 0
    if historical_data:
        recent_scores = historical_data.get('recent_scores', [])
        if recent_scores:
            historical_bonus = min(10, statistics.mean(recent_scores) / 10)
    
    # ç»¼åˆè¯„åˆ†
    final_score = (
        latency_score * WEIGHTS['latency'] +
        success_score * WEIGHTS['success_rate'] +
        stability_score * WEIGHTS['stability'] +
        speed_score * WEIGHTS['speed'] +
        historical_bonus
    )
    
    return round(final_score, 2)

# ========== æ–°å¢ï¼šä»å›ºå®š URL è·å–è®¢é˜…æº ==========
async def fetch_subscription_urls(session):
    """ä»å›ºå®š URL ä¸‹è½½è®¢é˜…æºåˆ—è¡¨ï¼Œæ›´æ–°å¹¶è¿”å› fallback URLs"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        async with session.get(UPDATE_FILE_URL, timeout=15, headers=headers) as resp:
            resp.raise_for_status()
            content = await resp.text()
            print(f"[DEBUG] åŸå§‹å†…å®¹: {content[:100]}...")
            if not content.strip():
                print(f"[âš ï¸] {UPDATE_FILE_URL} æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
                return load_fallback_urls()
            urls = [line.strip() for line in content.splitlines() if line.strip() and line.strip().startswith('http')]
            if urls:
                print(f"[âœ…] ä» {UPDATE_FILE_URL} è·å– {len(urls)} ä¸ªè®¢é˜…æº")
                save_fallback_urls(urls)
                return urls
            else:
                print(f"[âš ï¸] {UPDATE_FILE_URL} æ— æœ‰æ•ˆ URLï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
                return load_fallback_urls()
    except Exception as e:
        print(f"[âŒ] ä¸‹è½½ {UPDATE_FILE_URL} å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ° fallback URLs", file=sys.stderr)
        return load_fallback_urls()

# ========== ä»£ç†å¤„ç†å‡½æ•° ==========
async def fetch_subscription(session, url):
    """å¼‚æ­¥ä¸‹è½½è®¢é˜…å†…å®¹ï¼Œå¢åŠ é‡è¯•æœºåˆ¶"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for attempt in range(RETRY_COUNT):
        try:
            timeout = aiohttp.ClientTimeout(total=15 + attempt * 5)
            async with session.get(url, timeout=timeout, headers=headers) as resp:
                resp.raise_for_status()
                text = await resp.text()
                print(f"[DEBUG] è®¢é˜… {url} å†…å®¹é¦–100å­—ç¬¦: {text[:100]}...")
                return url, text
        except Exception as e:
            if attempt == RETRY_COUNT - 1:
                print(f"[âŒ] ä¸‹è½½å¤±è´¥ (é‡è¯•{RETRY_COUNT}æ¬¡): {url} é”™è¯¯: {e}", file=sys.stderr)
                return url, None
            else:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    return url, None

def validate_proxy_config(proxy):
    """éªŒè¯ä»£ç†é…ç½®çš„å®Œæ•´æ€§"""
    required_fields = ['name', 'type', 'server', 'port']
    for field in required_fields:
        if field not in proxy or not proxy[field]:
            return False
    
    # éªŒè¯ç«¯å£èŒƒå›´
    try:
        port = int(proxy['port'])
        if port <= 0 or port > 65535:
            return False
    except:
        return False
    
    # éªŒè¯æœåŠ¡å™¨åœ°å€
    server = proxy.get('server', '')
    if not server or server in ['localhost', '127.0.0.1', '0.0.0.0']:
        return False
    
    return True

def parse_clash_yaml(text):
    """è§£æ Clash YAML æ ¼å¼çš„è®¢é˜…ï¼Œå¢åŠ éªŒè¯"""
    try:
        data = yaml.safe_load(text)
        if isinstance(data, dict) and "proxies" in data:
            valid_proxies = []
            for proxy in data["proxies"]:
                if validate_proxy_config(proxy):
                    valid_proxies.append(proxy)
                else:
                    print(f"[âš ï¸] è·³è¿‡æ— æ•ˆé…ç½®: {proxy.get('name', 'unknown')}", file=sys.stderr)
            print(f"[DEBUG] è§£æåˆ° {len(valid_proxies)} ä¸ªæœ‰æ•ˆ Clash èŠ‚ç‚¹")
            return valid_proxies
    except Exception as e:
        print(f"[âš ï¸] è§£æ Clash YAML å¤±è´¥: {e}ï¼Œå†…å®¹: {text[:200]}...", file=sys.stderr)
    return []

def parse_base64_links(text):
    """ä¼˜åŒ–çš„ Base64 è§£æï¼Œå¢å¼ºå®¹é”™æ€§å’ŒèŠ‚ç‚¹éªŒè¯"""
    proxies = []
    uuid_count = {}
    seen_configs = set()
    
    try:
        # å¤šç§ Base64 è§£ç å°è¯•
        for encoding_attempt in [
            lambda x: base64.b64decode(x + "==="),
            lambda x: base64.b64decode(x.replace('-', '+').replace('_', '/') + "==="),
            lambda x: base64.urlsafe_b64decode(x + "===")
        ]:
            try:
                decoded_text = encoding_attempt(text.strip()).decode("utf-8", errors="ignore")
                break
            except:
                continue
        else:
            decoded_text = text.strip()
    except Exception as e:
        print(f"[âš ï¸] Base64 è§£ç å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬", file=sys.stderr)
        decoded_text = text.strip()

    for line in decoded_text.splitlines():
        line = line.strip()
        if not line:
            continue
            
        try:
            if line.startswith("vless://"):
                proxy_config = parse_vless_url(line)
                if proxy_config and validate_proxy_config(proxy_config):
                    # é¿å…é‡å¤é…ç½®
                    config_key = f"{proxy_config['server']}:{proxy_config['port']}:{proxy_config['uuid']}"
                    if config_key not in seen_configs:
                        seen_configs.add(config_key)
                        proxies.append(proxy_config)
                        
                        # UUID ä½¿ç”¨ç»Ÿè®¡
                        uuid = proxy_config['uuid']
                        uuid_count[uuid] = uuid_count.get(uuid, 0) + 1
                        
            elif line.startswith(("vmess://", "ss://", "trojan://")):
                # å¯ä»¥æ‰©å±•æ”¯æŒå…¶ä»–åè®®
                pass
                
        except Exception as e:
            print(f"[âš ï¸] è§£æèŠ‚ç‚¹é“¾æ¥å¤±è´¥: {line[:50]}... é”™è¯¯: {e}", file=sys.stderr)
    
    # æ£€æŸ¥ UUID é‡å¤ä½¿ç”¨æƒ…å†µ
    for uuid, count in uuid_count.items():
        if count > 10:
            print(f"[âš ï¸] UUID {uuid} é‡å¤ä½¿ç”¨ {count} æ¬¡ï¼Œå¯èƒ½å½±å“èŠ‚ç‚¹è´¨é‡", file=sys.stderr)
    
    print(f"[DEBUG] è§£æåˆ° {len(proxies)} ä¸ªæœ‰æ•ˆ vless èŠ‚ç‚¹")
    return proxies

def parse_vless_url(url):
    """è§£æå•ä¸ª vless URL"""
    try:
        url_part, *remark_part = url[8:].split("#", 1)
        base_name = urllib.parse.unquote(remark_part[0]) if remark_part else "vless"
        uuid, server_info = url_part.split("@", 1)
        server_port, *params_raw = server_info.split("?", 1)
        server, port = server_port.split(":", 1)
        params = urllib.parse.parse_qs(params_raw[0]) if params_raw else {}
        
        node_config = {
            "name": base_name,
            "type": "vless",
            "server": server,
            "port": int(port),
            "uuid": uuid,
            "network": params.get("type", ["tcp"])[0],
        }
        
        # WebSocket é…ç½®
        if node_config["network"] == "ws":
            path = params.get("path", [""])[0]
            if "proxyip:port(443)" in path:
                path = path.replace("proxyip:port(443)", f"{server}:{port}")
            ws_opts = {"path": path}
            if "host" in params:
                ws_opts["headers"] = {"Host": params["host"][0]}
            node_config["ws-opts"] = ws_opts
            
        # TLS é…ç½®
        if params.get("security", [""])[0] == "tls":
            node_config["tls"] = True
            if "sni" in params:
                node_config["servername"] = params["sni"][0]
        
        return node_config
    except Exception as e:
        print(f"[âš ï¸] è§£æ vless URL å¤±è´¥: {e}", file=sys.stderr)
        return None

def deduplicate(proxies):
    """å¢å¼ºçš„å»é‡é€»è¾‘ï¼Œè€ƒè™‘æ›´å¤šå› ç´ """
    seen = set()
    result = []
    for p in proxies:
        # ç”Ÿæˆæ›´ç²¾ç¡®çš„å»é‡é”®
        key_parts = [
            p.get('server', ''),
            str(p.get('port', 0)),
            p.get('type', ''),
            p.get('uuid', ''),
            p.get('network', 'tcp')
        ]
        
        if 'ws-opts' in p and p['ws-opts'].get('path'):
            key_parts.append(p['ws-opts']['path'])
        if p.get('servername'):
            key_parts.append(p['servername'])
            
        key = md5(':'.join(key_parts).encode()).hexdigest()
        if key not in seen:
            seen.add(key)
            result.append(p)
    
    print(f"[DEBUG] å»é‡åèŠ‚ç‚¹æ•°: {len(result)} (åŸå§‹: {len(proxies)})")
    return result

def filter_us(proxies):
    """å¢å¼ºçš„ US èŠ‚ç‚¹ç­›é€‰"""
    us_nodes = []
    exclude_keywords = [
        "HK", "HONG KONG", "é¦™æ¸¯", "æ¸¯",
        "SG", "SINGAPORE", "æ–°åŠ å¡", "ç‹®åŸ",
        "JP", "JAPAN", "æ—¥æœ¬", "ä¸œäº¬", "TOKYO",
        "KR", "KOREA", "éŸ©å›½", "é¦–å°”",
        "TW", "TAIWAN", "å°æ¹¾", "å°åŒ—",
        "CN", "CHINA", "ä¸­å›½", "å¤§é™†",
        "UK", "LONDON", "è‹±å›½", "ä¼¦æ•¦",
        "DE", "GERMANY", "å¾·å›½", "æ³•å…°å…‹ç¦",
        "FR", "FRANCE", "æ³•å›½", "å·´é»"
    ]
    
    us_keywords = [
        "US", "USA", "ç¾å›½", "UNITED STATES", "AMERICA",
        "LOS ANGELES", "NEW YORK", "CHICAGO", "DALLAS",
        "SAN FRANCISCO", "SEATTLE", "MIAMI", "DENVER",
        "VIRGINIA", "CALIFORNIA", "TEXAS", "OREGON"
    ]
    
    for p in proxies:
        name = p.get("name", "").upper()
        
        # å¿…é¡»åŒ…å« US å…³é”®è¯
        has_us_keyword = any(keyword in name for keyword in us_keywords)
        # ä¸èƒ½åŒ…å«æ’é™¤å…³é”®è¯
        has_exclude_keyword = any(exclude in name for exclude in exclude_keywords)
        
        if has_us_keyword and not has_exclude_keyword:
            us_nodes.append(p)
        elif has_exclude_keyword:
            print(f"[âš ï¸] æ’é™¤é US èŠ‚ç‚¹: {p['name']}", file=sys.stderr)
    
    print(f"[DEBUG] ç­›é€‰å‡º {len(us_nodes)} ä¸ª US èŠ‚ç‚¹")
    return us_nodes

def save_yaml(path, proxies):
    """ä¿å­˜ YAML æ–‡ä»¶ï¼Œå¢åŠ è´¨é‡ä¿¡æ¯"""
    abs_path = os.path.abspath(path)
    os.makedirs(os.path.dirname(abs_path), exist_ok=True)
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    sorted_proxies = sorted(proxies, key=lambda x: x.get('quality_score', 0), reverse=True)
    
    # æ·»åŠ å…ƒæ•°æ®
    output_data = {
        "proxies": sorted_proxies,
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "total_nodes": len(sorted_proxies),
            "quality_tested": sum(1 for p in sorted_proxies if 'quality_score' in p)
        }
    }
    
    with open(abs_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(output_data, f, allow_unicode=True, default_flow_style=False)
    
    print(f"[ğŸ’¾] å·²ä¿å­˜åˆ° {abs_path}ï¼ŒèŠ‚ç‚¹æ•°: {len(proxies)}")
    if sorted_proxies and 'quality_score' in sorted_proxies[0]:
        avg_score = statistics.mean([p['quality_score'] for p in sorted_proxies if 'quality_score' in p])
        print(f"[ğŸ“Š] å¹³å‡è´¨é‡åˆ†æ•°: {avg_score:.2f}")

# ========== å¢å¼ºçš„è¿æ¥æµ‹è¯• ==========
async def advanced_connection_test(session, proxy_config, test_urls=None):
    """é«˜çº§è¿æ¥æµ‹è¯•ï¼Œå¤šç»´åº¦è¯„ä¼°èŠ‚ç‚¹è´¨é‡"""
    if test_urls is None:
        test_urls = TEST_URLS
    
    node_name = proxy_config.get('name', 'æœªçŸ¥èŠ‚ç‚¹')
    server = proxy_config.get('server')
    port = int(proxy_config.get('port', 0))
    
    if not server or not port:
        return None
    
    # 1. Socket è¿æ¥æµ‹è¯•
    socket_latencies = []
    socket_success = 0
    
    for i in range(3):  # å¤šæ¬¡æµ‹è¯•æé«˜å‡†ç¡®æ€§
        latency = await test_socket_connection(server, port)
        if latency is not None:
            socket_latencies.append(latency)
            socket_success += 1
        await asyncio.sleep(0.1)  # å°é—´éš”
    
    if not socket_latencies or statistics.mean(socket_latencies) > LATENCY_THRESHOLD:
        print(f"[âŒ] {node_name} | Socket æµ‹è¯•å¤±è´¥æˆ–å»¶è¿Ÿè¿‡é«˜", file=sys.stderr)
        return None
    
    # 2. HTTP å“åº”æµ‹è¯•
    http_success = 0
    http_latencies = []
    
    for test_url in random.sample(test_urls, min(2, len(test_urls))):  # éšæœºæµ‹è¯•2ä¸ªURL
        try:
            start_time = time.time()
            timeout = aiohttp.ClientTimeout(total=TEST_TIMEOUT)
            async with session.get(test_url, timeout=timeout) as resp:
                await resp.read()  # ç¡®ä¿å®Œå…¨ä¸‹è½½
                latency = (time.time() - start_time) * 1000
                if resp.status in [200, 204]:
                    http_latencies.append(latency)
                    http_success += 1
        except Exception as e:
            print(f"[âš ï¸] {node_name} HTTP æµ‹è¯•å¤±è´¥: {e}", file=sys.stderr)
    
    # è®¡ç®—è´¨é‡åˆ†æ•°
    all_latencies = socket_latencies + http_latencies
    total_tests = 3 + len(test_urls)  # socketæµ‹è¯•3æ¬¡ + HTTPæµ‹è¯•
    success_count = socket_success + http_success
    
    if success_count / total_tests < SUCCESS_RATE_THRESHOLD:
        print(f"[âŒ] {node_name} | æˆåŠŸç‡è¿‡ä½ ({success_count}/{total_tests})", file=sys.stderr)
        return None
    
    # åŠ è½½å†å²æ•°æ®
    quality_history = load_quality_history()
    node_key = f"{server}:{port}"
    historical_data = quality_history.get(node_key, {})
    
    quality_score = calculate_quality_score(all_latencies, success_count, total_tests, historical_data)
    
    # æ›´æ–°å†å²æ•°æ®
    historical_data.setdefault('recent_scores', []).append(quality_score)
    historical_data['recent_scores'] = historical_data['recent_scores'][-10:]  # ä¿ç•™æœ€è¿‘10æ¬¡
    historical_data['last_test'] = datetime.now().isoformat()
    quality_history[node_key] = historical_data
    
    # æ·»åŠ è´¨é‡ä¿¡æ¯åˆ°ä»£ç†é…ç½®
    proxy_config['quality_score'] = quality_score
    proxy_config['test_info'] = {
        'avg_latency': round(statistics.mean(all_latencies), 2),
        'success_rate': round(success_count / total_tests, 3),
        'last_tested': datetime.now().isoformat()
    }
    
    print(f"[âœ…] {node_name} | è´¨é‡åˆ†æ•°: {quality_score:.2f} | å»¶è¿Ÿ: {statistics.mean(all_latencies):.0f}ms | æˆåŠŸç‡: {success_count}/{total_tests}")
    
    # ä¿å­˜æ›´æ–°çš„å†å²æ•°æ®
    save_quality_history(quality_history)
    
    return proxy_config

async def test_socket_connection(server, port, timeout=TEST_TIMEOUT):
    """å¼‚æ­¥ Socket è¿æ¥æµ‹è¯•"""
    try:
        loop = asyncio.get_running_loop()
        start_time = time.time()
        
        # ä½¿ç”¨ asyncio çš„è¿æ¥æµ‹è¯•
        try:
            _, writer = await asyncio.wait_for(
                asyncio.open_connection(server, port),
                timeout=timeout
            )
            end_time = time.time()
            writer.close()
            await writer.wait_closed()
            return (end_time - start_time) * 1000
        except asyncio.TimeoutError:
            return None
        except Exception:
            return None
            
    except Exception as e:
        return None

async def test_connection_async(session, proxy_config, semaphore):
    """å¼‚æ­¥æµ‹è¯•å•ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ€§ï¼Œä½¿ç”¨å¢å¼ºçš„æµ‹è¯•æ–¹æ³•"""
    async with semaphore:
        return await advanced_connection_test(session, proxy_config)

async def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«å®Œæ•´çš„ä¼˜åŒ–æµç¨‹"""
    all_proxies = []

    print("--- å¼€å§‹ä»å›ºå®š URL è·å–è®¢é˜…æº ---")
    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=100, ttl_dns_cache=300),
        timeout=aiohttp.ClientTimeout(total=30)
    ) as session:
        subscription_urls = await fetch_subscription_urls(session)
        if not subscription_urls:
            print("[âŒ] æ— å¯ç”¨è®¢é˜… URLï¼Œé€€å‡º", file=sys.stderr)
            return
        
        print("--- å¼€å§‹ä¸‹è½½å¹¶åˆå¹¶è®¢é˜… ---")
        tasks = [fetch_subscription(session, url) for url in subscription_urls]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in responses:
            if isinstance(result, Exception):
                print(f"[âŒ] ä¸‹è½½ä»»åŠ¡å¤±è´¥: {result}", file=sys.stderr)
                continue
                
            url, text = result
            if text:
                proxies = parse_clash_yaml(text) or parse_base64_links(text)
                if proxies:
                    print(f"[âœ…] è®¢é˜…: {url} â†’ {len(proxies)} èŠ‚ç‚¹")
                    all_proxies.extend(proxies)
                else:
                    print(f"[âš ï¸] æœªèƒ½è¯†åˆ«è®¢é˜…æ ¼å¼: {url}", file=sys.stderr)

    if not all_proxies:
        print("[âŒ] æœªè§£æåˆ°ä»»ä½•èŠ‚ç‚¹ï¼Œall.yaml å°†ä¸ºç©º", file=sys.stderr)
        save_yaml(OUTPUT_ALL, [])
        return

    # å»é‡å’ŒåŸºç¡€ç­›é€‰
    merged = deduplicate(all_proxies)
    print(f"[ğŸ“¦] åˆå¹¶å¹¶å»é‡åèŠ‚ç‚¹æ€»æ•°: {len(merged)}")
    
    # ä¿å­˜æ‰€æœ‰èŠ‚ç‚¹
    save_yaml(OUTPUT_ALL, merged)

    # ç­›é€‰ US èŠ‚ç‚¹
    us_nodes_to_test = filter_us(merged)
    if not us_nodes_to_test:
        print("[âš ï¸] æœªæ‰¾åˆ°ä»»ä½• US èŠ‚ç‚¹ï¼Œus.yaml å°†ä¸ºç©º")
        save_yaml(OUTPUT_US, [])
        return

    print(f"[ğŸ”] å¼€å§‹æµ‹è¯• {len(us_nodes_to_test)} ä¸ª US èŠ‚ç‚¹...")
    
    # è´¨é‡æµ‹è¯•
    available_us_nodes = []
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=100, ttl_dns_cache=300),
        timeout=aiohttp.ClientTimeout(total=30)
    ) as session:
        tasks = [test_connection_async(session, node, semaphore) for node in us_nodes_to_test]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    for result in results:
        if isinstance(result, Exception):
            print(f"[âš ï¸] èŠ‚ç‚¹æµ‹è¯•å¼‚å¸¸: {result}", file=sys.stderr)
            continue
        if result:
            available_us_nodes.append(result)

    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    available_us_nodes.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
    
    print(f"\n[âœ…] æµ‹è¯•å®Œæˆï¼è·å¾— {len(available_us_nodes)} ä¸ªé«˜è´¨é‡ US èŠ‚ç‚¹")
    
    if available_us_nodes:
        print(f"[ğŸ†] æœ€é«˜è´¨é‡èŠ‚ç‚¹: {available_us_nodes[0]['name']} (åˆ†æ•°: {available_us_nodes[0]['quality_score']:.2f})")
        avg_score = statistics.mean([node['quality_score'] for node in available_us_nodes])
        print(f"[ğŸ“Š] å¹³å‡è´¨é‡åˆ†æ•°: {avg_score:.2f}")
        save_yaml(OUTPUT_US, available_us_nodes)
    else:
        print("[âš ï¸] æ‰€æœ‰ US èŠ‚ç‚¹æµ‹è¯•å¤±è´¥ï¼Œus.yaml å°†ä¸ºç©º")
        save_yaml(OUTPUT_US, [])

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nè„šæœ¬å·²æ‰‹åŠ¨åœæ­¢ã€‚")
    except Exception as e:
        print(f"è„šæœ¬è¿è¡Œå‡ºé”™: {e}", file=sys.stderr)
